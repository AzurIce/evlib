{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E2VID Comprehensive Demo\n",
    "\n",
    "This notebook demonstrates the complete E2VID reconstruction pipeline with:\n",
    "- Multiple architecture options (UNet, FireNet, ONNX)\n",
    "- GPU acceleration\n",
    "- Quality metrics\n",
    "- Performance benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from pathlib import Path\n",
    "import evlib\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('default')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Event Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load slider_depth dataset\n",
    "events_path = \"../data/slider_depth/events.txt\"\n",
    "\n",
    "# Load a subset of events for faster processing\n",
    "max_events = 50000\n",
    "print(f\"Loading events from {events_path}...\")\n",
    "\n",
    "try:\n",
    "    events_data = np.loadtxt(events_path, max_rows=max_events)\n",
    "    ts = events_data[:, 0]\n",
    "    xs = events_data[:, 1].astype(np.int64)\n",
    "    ys = events_data[:, 2].astype(np.int64)\n",
    "    ps = events_data[:, 3].astype(np.int64)\n",
    "    \n",
    "    # Image dimensions from calib.txt\n",
    "    height, width = 180, 240\n",
    "    \n",
    "    print(f\"Loaded {len(xs)} events\")\n",
    "    print(f\"Time range: {ts[0]:.3f} - {ts[-1]:.3f} seconds\")\n",
    "    print(f\"Image dimensions: {width}x{height}\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"Dataset not found. Using synthetic events...\")\n",
    "    # Generate synthetic events\n",
    "    n_events = 10000\n",
    "    width, height = 256, 256\n",
    "    \n",
    "    xs = np.random.randint(0, width, n_events, dtype=np.int64)\n",
    "    ys = np.random.randint(0, height, n_events, dtype=np.int64)\n",
    "    ts = np.sort(np.random.uniform(0, 0.1, n_events))\n",
    "    ps = np.random.choice([-1, 1], n_events).astype(np.int64)\n",
    "    \n",
    "    print(f\"Generated {n_events} synthetic events\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Visualize Event Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create event histogram\n",
    "event_hist, _, _ = np.histogram2d(xs, ys, bins=[width, height], range=[[0, width], [0, height]])\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.imshow(event_hist.T, cmap='hot', origin='lower')\n",
    "plt.colorbar(label='Event count')\n",
    "plt.title('Event Density Map')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.hist(ts, bins=50, alpha=0.7)\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Event count')\n",
    "plt.title('Temporal Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Compare Different Reconstruction Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models to test\n",
    "models = [\"simple\", \"unet\", \"firenet\"]\n",
    "results = {}\n",
    "timings = {}\n",
    "\n",
    "# Test each model\n",
    "for model_type in models:\n",
    "    print(f\"\\nTesting {model_type} model...\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    frame = evlib.processing.events_to_video_advanced(\n",
    "        xs, ys, ts, ps, height, width,\n",
    "        num_bins=5,\n",
    "        model_type=model_type\n",
    "    )\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    \n",
    "    results[model_type] = frame\n",
    "    timings[model_type] = elapsed\n",
    "    \n",
    "    print(f\"  Time: {elapsed:.3f}s\")\n",
    "    print(f\"  Output shape: {frame.shape}\")\n",
    "    print(f\"  Value range: [{frame.min():.3f}, {frame.max():.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualize Reconstruction Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all results\n",
    "fig, axes = plt.subplots(1, len(models), figsize=(15, 5))\n",
    "\n",
    "for idx, (model_type, ax) in enumerate(zip(models, axes)):\n",
    "    frame = results[model_type]\n",
    "    im = ax.imshow(frame[:, :, 0], cmap='gray', vmin=0, vmax=1)\n",
    "    ax.set_title(f'{model_type.upper()}\\n({timings[model_type]:.3f}s)')\n",
    "    ax.axis('off')\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot difference images\n",
    "if len(models) > 1:\n",
    "    fig, axes = plt.subplots(1, len(models)-1, figsize=(10, 5))\n",
    "    if len(models) == 2:\n",
    "        axes = [axes]\n",
    "    \n",
    "    reference = results[\"simple\"]\n",
    "    \n",
    "    for idx, model_type in enumerate(models[1:]):\n",
    "        diff = np.abs(results[model_type] - reference)\n",
    "        im = axes[idx].imshow(diff[:, :, 0], cmap='hot', vmin=0, vmax=0.5)\n",
    "        axes[idx].set_title(f'{model_type.upper()} - Simple\\nDifference')\n",
    "        axes[idx].axis('off')\n",
    "        plt.colorbar(im, ax=axes[idx], fraction=0.046)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Quality Metrics Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate quality metrics between models\n",
    "def calculate_mse(img1, img2):\n",
    "    return np.mean((img1 - img2) ** 2)\n",
    "\n",
    "def calculate_psnr(img1, img2, max_val=1.0):\n",
    "    mse = calculate_mse(img1, img2)\n",
    "    if mse < 1e-10:\n",
    "        return 100.0\n",
    "    return 20 * np.log10(max_val / np.sqrt(mse))\n",
    "\n",
    "def calculate_ssim(img1, img2, c1=0.01**2, c2=0.03**2):\n",
    "    mu1, mu2 = np.mean(img1), np.mean(img2)\n",
    "    var1, var2 = np.var(img1), np.var(img2)\n",
    "    cov12 = np.mean((img1 - mu1) * (img2 - mu2))\n",
    "    \n",
    "    numerator = (2 * mu1 * mu2 + c1) * (2 * cov12 + c2)\n",
    "    denominator = (mu1**2 + mu2**2 + c1) * (var1 + var2 + c2)\n",
    "    \n",
    "    return numerator / denominator\n",
    "\n",
    "# Compare all models to simple baseline\n",
    "print(\"Quality Metrics (compared to Simple baseline):\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "reference = results[\"simple\"]\n",
    "metrics_data = []\n",
    "\n",
    "for model_type in models[1:]:\n",
    "    frame = results[model_type]\n",
    "    \n",
    "    mse = calculate_mse(frame, reference)\n",
    "    psnr = calculate_psnr(frame, reference)\n",
    "    ssim = calculate_ssim(frame[:, :, 0], reference[:, :, 0])\n",
    "    \n",
    "    metrics_data.append({\n",
    "        'Model': model_type.upper(),\n",
    "        'MSE': mse,\n",
    "        'PSNR (dB)': psnr,\n",
    "        'SSIM': ssim\n",
    "    })\n",
    "    \n",
    "    print(f\"\\n{model_type.upper()}:\")\n",
    "    print(f\"  MSE:  {mse:.4f}\")\n",
    "    print(f\"  PSNR: {psnr:.2f} dB\")\n",
    "    print(f\"  SSIM: {ssim:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance comparison\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Timing comparison\n",
    "model_names = list(timings.keys())\n",
    "times = list(timings.values())\n",
    "fps = [len(xs) / t for t in times]\n",
    "\n",
    "ax1.bar(model_names, times)\n",
    "ax1.set_ylabel('Time (seconds)')\n",
    "ax1.set_title('Reconstruction Time')\n",
    "ax1.set_ylim(0, max(times) * 1.2)\n",
    "\n",
    "for i, (name, t) in enumerate(zip(model_names, times)):\n",
    "    ax1.text(i, t + 0.01, f'{t:.3f}s', ha='center')\n",
    "\n",
    "# Throughput comparison\n",
    "ax2.bar(model_names, fps)\n",
    "ax2.set_ylabel('Events per second')\n",
    "ax2.set_title('Processing Throughput')\n",
    "ax2.set_ylim(0, max(fps) * 1.2)\n",
    "\n",
    "for i, (name, f) in enumerate(zip(model_names, fps)):\n",
    "    ax2.text(i, f + 100, f'{f:.0f}', ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary table\n",
    "print(\"\\nPerformance Summary:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Model':<10} {'Time (s)':<12} {'Events/sec':<15} {'Relative Speed':<15}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "simple_time = timings.get('simple', 1.0)\n",
    "for model, t in timings.items():\n",
    "    events_per_sec = len(xs) / t\n",
    "    relative_speed = simple_time / t\n",
    "    print(f\"{model:<10} {t:<12.3f} {events_per_sec:<15.0f} {relative_speed:<15.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Temporal Reconstruction (Video Sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruct multiple frames\n",
    "n_frames = 10\n",
    "time_window = (ts[-1] - ts[0]) / n_frames\n",
    "\n",
    "print(f\"Reconstructing {n_frames} frames...\")\n",
    "print(f\"Time window per frame: {time_window:.3f}s\")\n",
    "\n",
    "frames = []\n",
    "for i in range(n_frames):\n",
    "    t_start = ts[0] + i * time_window\n",
    "    t_end = t_start + time_window\n",
    "    \n",
    "    # Select events in time window\n",
    "    mask = (ts >= t_start) & (ts < t_end)\n",
    "    n_events_window = np.sum(mask)\n",
    "    \n",
    "    if n_events_window > 100:  # Minimum events for reconstruction\n",
    "        frame = evlib.processing.events_to_video_advanced(\n",
    "            xs[mask], ys[mask], ts[mask], ps[mask],\n",
    "            height, width,\n",
    "            model_type=\"firenet\"  # Use fast model for video\n",
    "        )\n",
    "        frames.append(frame[:, :, 0])\n",
    "    else:\n",
    "        # Use previous frame or black frame\n",
    "        frames.append(np.zeros((height, width)))\n",
    "\n",
    "print(f\"Reconstructed {len(frames)} frames\")\n",
    "\n",
    "# Display frames\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, (frame, ax) in enumerate(zip(frames, axes)):\n",
    "    ax.imshow(frame, cmap='gray', vmin=0, vmax=1)\n",
    "    ax.set_title(f'Frame {i+1}')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate temporal consistency\n",
    "if len(frames) > 1:\n",
    "    temporal_diffs = []\n",
    "    for i in range(1, len(frames)):\n",
    "        diff = calculate_mse(frames[i], frames[i-1])\n",
    "        temporal_diffs.append(diff)\n",
    "    \n",
    "    avg_temporal_diff = np.mean(temporal_diffs)\n",
    "    print(f\"\\nAverage temporal difference (MSE): {avg_temporal_diff:.4f}\")\n",
    "    print(f\"Temporal consistency score: {1 - avg_temporal_diff:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. GPU Acceleration Test (if available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test GPU acceleration\n",
    "# Note: This requires GPU support compiled into evlib\n",
    "\n",
    "print(\"Testing GPU acceleration...\")\n",
    "print(\"Note: GPU support requires evlib compiled with CUDA or Metal features\")\n",
    "\n",
    "# Try reconstruction with GPU preference\n",
    "# The library will automatically use GPU if available\n",
    "try:\n",
    "    start_gpu = time.time()\n",
    "    frame_gpu = evlib.processing.events_to_video_advanced(\n",
    "        xs, ys, ts, ps, height, width,\n",
    "        num_bins=5,\n",
    "        model_type=\"unet\"\n",
    "    )\n",
    "    time_gpu = time.time() - start_gpu\n",
    "    \n",
    "    print(f\"\\nReconstruction completed in {time_gpu:.3f}s\")\n",
    "    \n",
    "    # Compare with CPU timing\n",
    "    if \"unet\" in timings:\n",
    "        speedup = timings[\"unet\"] / time_gpu\n",
    "        print(f\"Potential speedup: {speedup:.2f}x\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"GPU test failed: {e}\")\n",
    "    print(\"This is expected if GPU support is not compiled in\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. ONNX Model Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test ONNX model loading\n",
    "onnx_model_path = \"../models/e2vid_lightweight.onnx\"\n",
    "\n",
    "if Path(onnx_model_path).exists():\n",
    "    print(f\"Testing ONNX model from {onnx_model_path}\")\n",
    "    \n",
    "    try:\n",
    "        start_onnx = time.time()\n",
    "        frame_onnx = evlib.processing.events_to_video_advanced(\n",
    "            xs, ys, ts, ps, height, width,\n",
    "            num_bins=5,\n",
    "            model_type=\"onnx\",\n",
    "            model_path=onnx_model_path\n",
    "        )\n",
    "        time_onnx = time.time() - start_onnx\n",
    "        \n",
    "        print(f\"ONNX inference completed in {time_onnx:.3f}s\")\n",
    "        \n",
    "        # Display result\n",
    "        plt.figure(figsize=(8, 4))\n",
    "        plt.subplot(121)\n",
    "        plt.imshow(frame_onnx[:, :, 0], cmap='gray')\n",
    "        plt.title('ONNX Model Output')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(122)\n",
    "        if \"unet\" in results:\n",
    "            diff = np.abs(frame_onnx - results[\"unet\"])\n",
    "            plt.imshow(diff[:, :, 0], cmap='hot')\n",
    "            plt.title('ONNX vs UNet Difference')\n",
    "            plt.colorbar()\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ONNX model test failed: {e}\")\n",
    "else:\n",
    "    print(f\"ONNX model not found at {onnx_model_path}\")\n",
    "    print(\"Run 'python examples/download_pretrained_models.py' to download models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "1. **Multiple reconstruction architectures**: Simple, UNet, and FireNet\n",
    "2. **Performance comparison**: FireNet is fastest, UNet provides best quality\n",
    "3. **Quality metrics**: MSE, PSNR, and SSIM for objective evaluation\n",
    "4. **Temporal reconstruction**: Creating video sequences from events\n",
    "5. **GPU acceleration**: Automatic GPU usage when available\n",
    "6. **ONNX support**: Loading pre-trained models in ONNX format\n",
    "\n",
    "The evlib library provides a comprehensive toolkit for event-to-video reconstruction with state-of-the-art algorithms!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
