{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Event Camera Data Visualization with evlib\n\nThis notebook provides a comprehensive guide to visualizing event camera data using both traditional approaches and the modern evlib library. Event cameras capture changes in pixel intensity rather than absolute values, resulting in sparse, asynchronous data streams with high temporal resolution.\n\nWe demonstrate multiple visualization techniques including:\n- Basic event data loading and 2D visualization\n- Time surfaces and temporal representations\n- 3D spatiotemporal plots\n- Voxel grids and smooth representations\n- Event transformations and augmentations\n- Synthetic data generation and real data analysis\n\nThe notebook uses both real event data from the slider_depth dataset and synthetic data to illustrate different concepts.",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Introduction\n\nEvent cameras, also known as Dynamic Vision Sensors (DVS), detect brightness changes asynchronously at each pixel. Unlike traditional cameras that capture full frames at fixed intervals, event cameras generate sparse streams of events only when brightness changes exceed a threshold.\n\nEach event contains four components:\n- **t**: timestamp (when the event occurred)\n- **x**: horizontal pixel coordinate\n- **y**: vertical pixel coordinate  \n- **p**: polarity (1 for brightness increase, 0 for brightness decrease)\n\nThis notebook demonstrates various approaches to visualize and analyze event data using the evlib library alongside traditional Python tools.",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Data Format\n\nEvent data is typically stored in text format with one event per line:\n\n```\n0.003811000 96 133 0\n0.003820001 127 171 0\n0.003836000 4 160 0\n```\n\nFormat: `timestamp x y polarity`\n\nFor the DAVIS240C camera (240×180 resolution):\n- x ∈ {0, ..., 239} (column index)\n- y ∈ {0, ..., 179} (row index)  \n- p ∈ {0, 1} (0=negative, 1=positive brightness change)",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom matplotlib.animation import FuncAnimation\nimport evlib\n\n# Set random seed for reproducibility\nnp.random.seed(42)\n\nprint(\"evlib imports available:\")\nprint(f\"- formats: {hasattr(evlib, 'formats')}\")\nprint(f\"- representations: {hasattr(evlib, 'representations')}\")\nprint(f\"- stacked_histogram: {hasattr(evlib, 'stacked_histogram')}\")\nprint(f\"- create_voxel_grid: {hasattr(evlib, 'create_voxel_grid')}\")\nprint(f\"- create_time_surface: {hasattr(evlib, 'create_time_surface')}\")\nprint(f\"NumPy version: {np.__version__}\")\nprint(f\"Matplotlib version: {plt.matplotlib.__version__}\")"
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Real Event Data Loading\n\nLet's load real event data from the slider_depth dataset to demonstrate event visualization techniques.",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load real event data from slider_depth dataset\nfilename_sub = '../data/slider_depth/events.txt'\n\n# Check if file exists\nif not os.path.exists(filename_sub):\n    raise FileNotFoundError(f\"Real data file not found: {filename_sub}\")\n\n# Load event data using standard Python approach (could be replaced with evlib loader when available)\nwith open(filename_sub, 'r') as f:\n    lines = f.readlines()\n\ntimestamp = []\nx = []\ny = []\npol = []\n\nfor line in lines:\n    parts = line.strip().split()\n    if len(parts) == 4:\n        timestamp.append(float(parts[0]))\n        x.append(int(parts[1]))\n        y.append(int(parts[2]))\n        pol.append(int(parts[3]))\n\ntimestamp = np.array(timestamp)\nx = np.array(x)\ny = np.array(y)\npol = np.array(pol)\n\nprint(f\"Loaded {len(timestamp)} events from real dataset\")\nprint(f\"Time range: {timestamp[0]:.6f} to {timestamp[-1]:.6f} seconds\")\nprint(f\"Duration: {timestamp[-1] - timestamp[0]:.3f} seconds\")\nprint(f\"Spatial range: x=[{x.min()}, {x.max()}], y=[{y.min()}, {y.max()}]\")\nprint(f\"Average event rate: {len(timestamp) / (timestamp[-1] - timestamp[0]):.0f} events/second\")\nprint(f\"Polarity distribution: {np.sum(pol == 1)}/{len(pol)} positive events ({100*np.sum(pol == 1)/len(pol):.1f}%)\")\n\n# Define sensor size for DAVIS240C\nimg_size = (180, 240)  # height, width\nsensor_resolution = (240, 180)  # width, height (for evlib API)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Basic Event Visualization\n\nLet's start with basic 2D visualization of events, showing how polarity affects the display.",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_events_2d(xs, ys, ps, title=\"Events\", ax=None, sensor_size=None):\n    \"\"\"Plot events in 2D with color representing polarity\"\"\"\n    if ax is None:\n        fig, ax = plt.subplots(figsize=(8, 6))\n        \n    # Color by polarity (red=positive, blue=negative)\n    colors = ['r' if p > 0 else 'b' for p in ps]\n    \n    ax.scatter(xs, ys, c=colors, s=2, alpha=0.6)\n    ax.set_title(title)\n    ax.set_xlabel('X [pixels]')\n    ax.set_ylabel('Y [pixels]')\n    \n    if sensor_size is not None:\n        ax.set_xlim(0, sensor_size[1])  # width\n        ax.set_ylim(0, sensor_size[0])  # height\n    \n    ax.invert_yaxis()  # Invert Y axis to match image coordinates\n    ax.grid(True, alpha=0.3)\n    \n    return ax\n\n# Plot 2D event data at different temporal windows\nfig, axes = plt.subplots(1, 3, figsize=(18, 6))\n\n# Early events\nplot_events_2d(x[:2000], y[:2000], pol[:2000], \n               \"Early Events (First 2000)\", axes[0], img_size)\n\n# Middle events  \nmid_start = len(x) // 2\nplot_events_2d(x[mid_start:mid_start+2000], y[mid_start:mid_start+2000], pol[mid_start:mid_start+2000], \n               \"Middle Events\", axes[1], img_size)\n\n# Recent events\nplot_events_2d(x[-2000:], y[-2000:], pol[-2000:], \n               \"Recent Events (Last 2000)\", axes[2], img_size)\n\nplt.tight_layout()\nplt.show()\n\n# Show overall event distribution\nfig, ax = plt.subplots(figsize=(10, 8))\nplot_events_2d(x[::50], y[::50], pol[::50], \n               f\"Event Distribution Overview (Every 50th Event)\", ax, img_size)\nplt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Event Representations using evlib\n\nThe evlib library provides efficient implementations for common event camera representations.",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "num_events = 5000\nprint(f\"Event polarity balance visualization: num_events = {num_events}\")\n\n# Convert polarity from {0,1} to {-1,1} for signed representation\npol_signed = 2 * pol[:num_events] - 1\n\n# Create polarity balance image\nimg = np.zeros(img_size, dtype=np.int32)\nfor i in range(min(num_events, len(pol))):\n    if 0 <= y[i] < img_size[0] and 0 <= x[i] < img_size[1]:\n        img[y[i], x[i]] += pol_signed[i]\n\n# Visualize polarity balance\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n\n# Grayscale visualization\nmaxabsval = np.amax(np.abs(img))\nax1.imshow(img, cmap='gray', clim=(-maxabsval, maxabsval))\nax1.set_title('Balance of Event Polarities (Grayscale)')\nax1.set_xlabel(\"X [pixels]\")\nax1.set_ylabel(\"Y [pixels]\")\n\n# Red-blue visualization  \nax2.imshow(img, cmap='seismic_r', clim=(-maxabsval, maxabsval))\nax2.set_title('Balance of Event Polarities (Red-Blue)')\nax2.set_xlabel(\"X [pixels]\")\nax2.set_ylabel(\"Y [pixels]\")\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"Maximum positive accumulation: {img.max()}\")\nprint(f\"Maximum negative accumulation: {abs(img.min())}\")\nprint(f\"Active pixels: {np.sum(img != 0)} / {np.prod(img_size)} ({100*np.sum(img != 0)/np.prod(img_size):.1f}%)\")\n\n# Use evlib for stacked histogram representation\n# Create stacked histogram using evlib\nnum_bins = 5\nstacked_hist = evlib.stacked_histogram(\n    x[:num_events], \n    y[:num_events], \n    pol[:num_events], \n    timestamp[:num_events],\n    bins=num_bins,\n    height=img_size[0],  # sensor height\n    width=img_size[1],   # sensor width\n    count_cutoff=255,\n    fastmode=True\n)\n\nprint(f\"Created stacked histogram with shape: {stacked_hist.shape}\")\nprint(f\"Stacked histogram value range: [{stacked_hist.min()}, {stacked_hist.max()}]\")\n\n# Visualize middle time slice for positive events\nfig, axes = plt.subplots(1, 3, figsize=(18, 6))\n\n# Positive events (first half of channels)\npos_slice = stacked_hist[num_bins//2, :, :]  # Middle time bin, positive events\naxes[0].imshow(pos_slice, cmap='Reds')\naxes[0].set_title(f'Stacked Histogram - Positive Events (Time Bin {num_bins//2})')\naxes[0].set_xlabel(\"X [pixels]\")\naxes[0].set_ylabel(\"Y [pixels]\")\n\n# Negative events (second half of channels)\nneg_slice = stacked_hist[num_bins + num_bins//2, :, :]  # Middle time bin, negative events\naxes[1].imshow(neg_slice, cmap='Blues')\naxes[1].set_title(f'Stacked Histogram - Negative Events (Time Bin {num_bins//2})')\naxes[1].set_xlabel(\"X [pixels]\")\naxes[1].set_ylabel(\"Y [pixels]\")\n\n# Combined view (difference between positive and negative)\ncombined_slice = pos_slice.astype(np.float32) - neg_slice.astype(np.float32)\naxes[2].imshow(combined_slice, cmap='RdBu_r')\naxes[2].set_title(f'Combined View (Pos - Neg, Time Bin {num_bins//2})')\naxes[2].set_xlabel(\"X [pixels]\")\naxes[2].set_ylabel(\"Y [pixels]\")\n\nplt.tight_layout()\nplt.show()\n\n# Also create a voxel grid for comparison\nvoxel_grid = evlib.create_voxel_grid(\n    x[:num_events], \n    y[:num_events], \n    timestamp[:num_events], \n    pol[:num_events],\n    sensor_resolution=(img_size[1], img_size[0]),  # (width, height)\n    num_bins=num_bins,\n    t_min=timestamp[0],\n    t_max=timestamp[min(num_events-1, len(timestamp)-1)]\n)\n\nprint(f\"Created voxel grid with shape: {voxel_grid.shape}\")\nprint(f\"Voxel value range: [{voxel_grid.min():.3f}, {voxel_grid.max():.3f}]\")\n\n# Visualize middle time slice of voxel grid\nfig, ax = plt.subplots(figsize=(8, 6))\nmiddle_slice = voxel_grid[:, :, num_bins//2]\nax.imshow(middle_slice.T, cmap='RdBu_r')\nax.set_title(f'Voxel Grid (Time Slice {num_bins//2})')\nax.set_xlabel(\"X [pixels]\")\nax.set_ylabel(\"Y [pixels]\")\nplt.colorbar(ax.images[0], ax=ax)\nplt.show()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Event Transformations using evlib\n\nevlib provides efficient implementations for common event transformations including spatial flips, rotations, and augmentations.",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "def create_time_surface(xs, ys, ts, ps, sensor_size, decay=0.05):\n    \"\"\"Create a time surface representation from events\"\"\"\n    time_surface_pos = np.zeros(sensor_size)\n    time_surface_neg = np.zeros(sensor_size)\n    \n    # Get latest timestamp\n    latest_t = ts.max()\n    \n    # Process events\n    for x, y, t, p in zip(xs, ys, ts, ps):\n        if 0 <= x < sensor_size[1] and 0 <= y < sensor_size[0]:\n            if p > 0:\n                time_surface_pos[y, x] = np.exp(-(latest_t - t) / decay)\n            else:\n                time_surface_neg[y, x] = np.exp(-(latest_t - t) / decay)\n    \n    # Create RGB image: R=positive events, B=negative events\n    time_surface_rgb = np.zeros((*sensor_size, 3))\n    time_surface_rgb[:, :, 0] = time_surface_pos  # Red channel for positive events\n    time_surface_rgb[:, :, 2] = time_surface_neg  # Blue channel for negative events\n    \n    return time_surface_rgb\n\n# Demonstrate event transformations using manual implementations\n# NOTE: evlib.augmentation functions are not currently available\nsample_size = 2000\nsample_x = x[:sample_size]\nsample_y = y[:sample_size] \nsample_t = timestamp[:sample_size]\nsample_p = pol[:sample_size]\n\n# Manual flip X-axis transformation\ndef flip_events_x(xs, ys, ts, ps, sensor_size):\n    \"\"\"Flip events along X-axis\"\"\"\n    width, height = sensor_size\n    flipped_xs = width - 1 - xs\n    return flipped_xs, ys, ts, ps\n\n# Manual flip Y-axis transformation  \ndef flip_events_y(xs, ys, ts, ps, sensor_size):\n    \"\"\"Flip events along Y-axis\"\"\"\n    width, height = sensor_size\n    flipped_ys = height - 1 - ys\n    return xs, flipped_ys, ts, ps\n\n# Manual rotation transformation\ndef rotate_events(xs, ys, ts, ps, sensor_size, theta_radians, center):\n    \"\"\"Rotate events around a center point\"\"\"\n    cx, cy = center\n    cos_theta = np.cos(theta_radians)\n    sin_theta = np.sin(theta_radians)\n    \n    # Translate to origin\n    x_centered = xs - cx\n    y_centered = ys - cy\n    \n    # Rotate\n    rotated_x = x_centered * cos_theta - y_centered * sin_theta\n    rotated_y = x_centered * sin_theta + y_centered * cos_theta\n    \n    # Translate back\n    final_x = rotated_x + cx\n    final_y = rotated_y + cy\n    \n    return final_x, final_y, ts, ps\n\n# Apply transformations\nflipped_x_xs, flipped_x_ys, flipped_x_ts, flipped_x_ps = flip_events_x(\n    sample_x, sample_y, sample_t, sample_p, (img_size[1], img_size[0]))\n\nflipped_y_xs, flipped_y_ys, flipped_y_ts, flipped_y_ps = flip_events_y(\n    sample_x, sample_y, sample_t, sample_p, (img_size[1], img_size[0]))\n\ntheta_radians = np.pi / 4  # 45 degrees\ncenter = (img_size[1] // 2, img_size[0] // 2)  # (width//2, height//2)\nrotated_xs, rotated_ys, rotated_ts, rotated_ps = rotate_events(\n    sample_x, sample_y, sample_t, sample_p, \n    (img_size[1], img_size[0]), theta_radians, center)\n\n# Filter out events that are out of bounds after rotation\nvalid_mask = ((rotated_xs >= 0) & (rotated_xs < img_size[1]) & \n              (rotated_ys >= 0) & (rotated_ys < img_size[0]))\nrotated_xs = rotated_xs[valid_mask]\nrotated_ys = rotated_ys[valid_mask]\nrotated_ts = rotated_ts[valid_mask]\nrotated_ps = rotated_ps[valid_mask]\n\n# Create time surfaces for each transformation\nts_original = create_time_surface(sample_x, sample_y, sample_t, sample_p, img_size)\nts_flipped_x = create_time_surface(flipped_x_xs, flipped_x_ys, flipped_x_ts, flipped_x_ps, img_size)\nts_flipped_y = create_time_surface(flipped_y_xs, flipped_y_ys, flipped_y_ts, flipped_y_ps, img_size)\nts_rotated = create_time_surface(rotated_xs.astype(int), rotated_ys.astype(int), rotated_ts, rotated_ps, img_size)\n\n# Visualize transformations\nfig, axes = plt.subplots(2, 2, figsize=(12, 10))\n\naxes[0, 0].imshow(ts_original)\naxes[0, 0].set_title(\"Original Events\")\naxes[0, 0].axis('off')\n\naxes[0, 1].imshow(ts_flipped_x)\naxes[0, 1].set_title(\"Flipped X-axis\")\naxes[0, 1].axis('off')\n\naxes[1, 0].imshow(ts_flipped_y)\naxes[1, 0].set_title(\"Flipped Y-axis\")\naxes[1, 0].axis('off')\n\naxes[1, 1].imshow(ts_rotated)\naxes[1, 1].set_title(\"Rotated 45°\")\naxes[1, 1].axis('off')\n\nplt.suptitle(\"Event Transformations (Manual Implementation)\", fontsize=14)\nplt.tight_layout()\nplt.show()\n\nprint(\"Successfully demonstrated event transformation functions:\")\nprint(f\"- Original events: {len(sample_x)}\")\nprint(f\"- Flipped X events: {len(flipped_x_xs)}\")\nprint(f\"- Flipped Y events: {len(flipped_y_xs)}\")\nprint(f\"- Rotated events: {len(rotated_xs)} (after filtering out-of-bounds)\")\n\n# Use evlib time surface function if available\nevlib_time_surface = evlib.create_time_surface(\n    sample_x, sample_y, sample_t, sample_p,\n    sensor_resolution=(img_size[1], img_size[0]),\n    decay_constant=0.05,\n    polarity_separate=True\n)\n\nprint(f\"Created evlib time surface with shape: {evlib_time_surface.shape}\")\n\n# Visualize evlib time surface\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\naxes[0].imshow(evlib_time_surface[0], cmap='Blues')\naxes[0].set_title(\"Time Surface - Negative Events (evlib)\")\naxes[0].axis('off')\n\naxes[1].imshow(evlib_time_surface[1], cmap='Reds')\naxes[1].set_title(\"Time Surface - Positive Events (evlib)\")\naxes[1].axis('off')\n\nplt.tight_layout()\nplt.show()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Answers for color map visualization:\n# - White pixels: areas with strong positive event accumulation (brightness increase)\n# - Red pixels: areas with moderate positive event accumulation\n# - Blue pixels: areas with negative event accumulation (brightness decrease)\n# - Blue is used instead of green because red-blue provides better contrast for \n#   opposing polarities and is a common diverging colormap for signed data\n\n# Ternary image (last event polarity per pixel)\nimg_ternary = np.zeros(img_size, np.int32)\nfor i in range(num_events):\n    img_ternary[y[i], x[i]] = (2 * pol[i] - 1)  # Convert to {-1, +1}\n\n# Display the ternary image\nfig = plt.figure(figsize=(8, 6))\nfig.suptitle('Last event polarity per pixel')\nplt.imshow(img_ternary, cmap='gray', vmin=-1, vmax=1)\nplt.xlabel(\"x [pixels]\")\nplt.ylabel(\"y [pixels]\")\nplt.colorbar()\nplt.show()\n\n# This representation is good for:\n# - Reduced memory usage (only 3 values per pixel)\n# - Clear edge visualization\n# - Fast processing for real-time applications\n# - Input to some event-based algorithms that only need polarity information"
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "idx_pos = pol[:num_events] > 0\nidx_neg = ~idx_pos\n\nimg_pos = np.zeros(img_size, dtype=np.int32)\nimg_neg = np.zeros(img_size, dtype=np.int32)\n\nfor i in range(num_events):\n    if pol[i] > 0:\n        img_pos[y[i], x[i]] += 1\n    else:\n        img_neg[y[i], x[i]] += 1\n\nfig = plt.figure(figsize=(8, 6))\nfig.suptitle('Histogram of positive events')\nplt.imshow(img_pos, cmap='viridis')\nplt.xlabel(\"x [pixels]\")\nplt.ylabel(\"y [pixels]\")\nplt.colorbar()\nplt.show()\n\nfig = plt.figure(figsize=(8, 6))\nfig.suptitle('Histogram of negative events')\nplt.imshow(img_neg, cmap='viridis')\nplt.xlabel(\"x [pixels]\")\nplt.ylabel(\"y [pixels]\")\nplt.colorbar()\nplt.show()\n\nprint(f\"Max positive events at a pixel: {img_pos.max()}\")\nprint(f\"Max negative events at a pixel: {img_neg.max()}\")\nprint(f\"Total positive events: {img_pos.sum()}\")\nprint(f\"Total negative events: {img_neg.sum()}\")"
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "num_events = 50000\nprint(\"Time surface: num_events =\", num_events)\n\ntau = 0.03\nt_ref = timestamp[min(num_events-1, len(timestamp)-1)]\n\nts_image = np.zeros(img_size, dtype=np.float64)\nfor i in range(min(num_events, len(timestamp))):\n    ts_image[y[i], x[i]] = timestamp[i]\n\ntime_surface = np.exp(-(t_ref - ts_image) / tau)\ntime_surface[ts_image == 0] = 0\n\nfig = plt.figure(figsize=(8, 6))\nfig.suptitle('Time surface (exp decay). Both polarities')\nplt.imshow(time_surface, cmap='hot')\nplt.xlabel(\"x [pixels]\")\nplt.ylabel(\"y [pixels]\")\nplt.colorbar()\nplt.show()\n\nts_pos = np.zeros(img_size, dtype=np.float64)\nts_neg = np.zeros(img_size, dtype=np.float64)\n\nfor i in range(min(num_events, len(timestamp))):\n    if pol[i] > 0:\n        ts_pos[y[i], x[i]] = timestamp[i]\n    else:\n        ts_neg[y[i], x[i]] = timestamp[i]\n\nsae_pos = np.exp(-(t_ref - ts_pos) / tau)\nsae_pos[ts_pos == 0] = 0\n\nsae_neg = np.exp(-(t_ref - ts_neg) / tau)\nsae_neg[ts_neg == 0] = 0\n\nfig = plt.figure(figsize=(8, 6))\nfig.suptitle('Time surface (exp decay) of positive events')\nplt.imshow(sae_pos, cmap='hot')\nplt.xlabel(\"x [pixels]\")\nplt.ylabel(\"y [pixels]\")\nplt.colorbar()\nplt.show()\n\nfig = plt.figure(figsize=(8, 6))\nfig.suptitle('Time surface (exp decay) of negative events')\nplt.imshow(sae_neg, cmap='hot')\nplt.xlabel(\"x [pixels]\")\nplt.ylabel(\"y [pixels]\")\nplt.colorbar()\nplt.show()\n\nprint(f\"Positive events show {np.sum(sae_pos > 0)} active pixels\")\nprint(f\"Negative events show {np.sum(sae_neg > 0)} active pixels\")"
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, consider using [pseudocolor](https://en.wikipedia.org/wiki/False_color) to display the events. Write code to generate the following image.\n",
    "\n",
    "![balance_polarities_red_blue](images/balance_polarities_red_blue.png)\n",
    "\n",
    "- What do the white, red and blue colored pixels represent?\n",
    "- Why is blue color used instead of green color?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "print(\"Exploring additional event representations...\")\n\n# Create polarity-based colored image\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\n# Simple polarity visualization\nn_vis = 5000\nimg_pol = np.zeros((*img_size, 3))\nfor i in range(min(n_vis, len(timestamp))):\n    if pol[i] > 0:\n        img_pol[y[i], x[i], 0] = 1.0  # Red for positive\n    else:\n        img_pol[y[i], x[i], 2] = 1.0  # Blue for negative\n\naxes[0].imshow(img_pol)\naxes[0].set_title(\"Polarity-based coloring\")\naxes[0].axis('off')\n\n# Time-based coloring\nimg_time = np.zeros(img_size)\nfor i in range(min(n_vis, len(timestamp))):\n    # Normalize time to [0, 1]\n    t_norm = (timestamp[i] - timestamp[0]) / (timestamp[min(n_vis-1, len(timestamp)-1)] - timestamp[0])\n    img_time[y[i], x[i]] = t_norm\n\naxes[1].imshow(img_time, cmap='viridis')\naxes[1].set_title(\"Time-based coloring\")\naxes[1].axis('off')\n\n# Combined polarity-time coloring\nimg_combined = np.zeros((*img_size, 3))\nfor i in range(min(n_vis, len(timestamp))):\n    t_norm = (timestamp[i] - timestamp[0]) / (timestamp[min(n_vis-1, len(timestamp)-1)] - timestamp[0])\n    if pol[i] > 0:\n        img_combined[y[i], x[i], 0] = t_norm  # Red intensity by time\n    else:\n        img_combined[y[i], x[i], 2] = t_norm  # Blue intensity by time\n\naxes[2].imshow(img_combined)\naxes[2].set_title(\"Polarity-time coloring\")\naxes[2].axis('off')\n\nplt.tight_layout()\nplt.show()\n\n# Event stream statistics\nprint(\"Event stream statistics:\")\nprint(f\"Total events loaded: {len(timestamp)}\")\nprint(f\"Duration: {timestamp[-1] - timestamp[0]:.3f} seconds\")\nprint(f\"Average event rate: {len(timestamp) / (timestamp[-1] - timestamp[0]):.0f} events/second\")\nprint(f\"Positive/Negative ratio: {np.sum(pol) / len(pol):.3f}\")\n\n# Spatial activity distribution\nspatial_activity = np.zeros(img_size, dtype=np.int32)\nfor i in range(len(x)):\n    spatial_activity[y[i], x[i]] += 1\n\nprint(f\"Active pixels: {np.sum(spatial_activity > 0)} / {np.prod(img_size)} ({100*np.sum(spatial_activity > 0)/np.prod(img_size):.1f}%)\")\nprint(f\"Max events per pixel: {spatial_activity.max()}\")\nprint(f\"Mean events per active pixel: {np.mean(spatial_activity[spatial_activity > 0]):.1f}\")\n\nprint(\"Event visualization and analysis complete!\")",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Images of timestamps\n",
    "\n",
    "Other useful representations are those that, instead of counting events at every pixel, just show a timestamp per pixel. It could be the last timestamp, an average timestamp or a timestamp composed with an exponential decay.\n",
    "They are also called \"time surfaces\", \"time maps\" or \"surface of active events\".\n",
    "\n",
    "### Time maps with exponential decay\n",
    "\n",
    "Next, write code to replicate the type of time-images of the [2015 PAMI paper HOTS, Fig. 2](https://www.neuromorphic-vision.com/public/publications/1/publication.pdf).\n",
    "Use 50000 events to better see the traces of the edges as they trigger events when they move through the image plane.\n",
    "\n",
    "$ image(x,y; t) = exp(-|t-T(x,y)| / \\tau) $\n",
    "\n",
    "This is equation (3) in the above paper. The paper uses $\\tau = 50$ ms (see the bottom of page 8). For the plots below, a value $\\tau = 30$ ms is used. $\\tau$ is a tunable parameter that depends on the motion in the scene.\n",
    "Note that the paper uses a neighborhood (\"context\") of size $(2R+1)\\times (2R+1)$ centered at the last event. Instead, you are aked to visualize the time surface using all image pixels (not a subset of them).\n",
    "\n",
    "Also note that the paper uses the polarity $p$ as an argument of the time map $T$, while here it is not explicitly written. The following figure shows both polarities on the same time map, which is not easy to write with such a notation.\n",
    "\n",
    "![](images/ts_exp_pol.png)\n",
    "\n",
    "Next, you are asked to split by polarity, creating one plot for each event polarity:\n",
    "\n",
    "![](images/ts_exp_pos.png)\n",
    "![](images/ts_exp_neg.png)\n",
    "\n",
    "- Describe what you see in this representation and whether it is better or not to split by polarity. In what situations?\n",
    "- Is there the same amount of noise on both type of events (positive, negative)?\n",
    "\n",
    "<!-- ![](images/ts_exp_pol_red_blue.png) \n",
    "![](images/ts_exp_balance_pol_red_blue.png) -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average timestamp images.\n",
    "In this type of representation, each pixel contains the average timestamp of the events that happened in it in the last few milliseconds. (There is no exponential decay, just an average).\n",
    "\n",
    "![](images/t_ave_both_pols.png)\n",
    "\n",
    "Next, split by polarity:\n",
    "\n",
    "![](images/t_ave_pos.png)\n",
    "![](images/t_ave_neg.png)\n",
    "\n",
    "- Describe what you oberve compared to previous representations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Space-time plots\n",
    "\n",
    "Next, we take a look at different ways to visualize events in space-time. Such a space-time is obtained by considering a temporal window of the signal that is output by the camera as a response of the light arriving at the image plane (i.e., the \"retina\").\n",
    "\n",
    "## Point set\n",
    "\n",
    "Write Python code to plot the first $N_e = 2000$ events in space-time, like a point set or point \"cloud\":\n",
    "\n",
    "![Events, space-time and polarity](images/space_time_pol.png)\n",
    "\n",
    "You may find the [3D scatterplot example](https://matplotlib.org/3.1.1/gallery/mplot3d/scatter3d.html) useful.\n",
    "\n",
    "Try experimenting by moving around the viewpoint (clicking on the figure generated by Python's matplotlib).\n",
    "\n",
    "- For us, humans, is the information more intelligible from any specific viewpoint?\n",
    "- What information do you gain by looking at this point cloud from directions parallel to each of the coordinate axes?\n",
    "\n",
    "Then, write a function to generate a short **[movie](movie.mp4)** that smoothly shows the intermediate viewpoints between two specified ones (the start and end viewpoints). See the following VLC player screenshot.\n",
    "Suggestion: use the matplotlib command `ax.view_init(azim=XXX,elev=ZZZ)`. Write images to disk and create a movie from them using ffmpeg. There is no need to install ffmpeg; you may use the [static build](https://johnvansickle.com/ffmpeg/). Video [coding options](https://trac.ffmpeg.org/wiki/Encode/H.264), such as lossless.\n",
    "\n",
    "![movie snapthot on VLC](images/movie_vlc_pointset.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voxel grid representation\n",
    "\n",
    "Next, you are asked to write code to convert the event data into a 3D histogram. To this end, events shall be collected into bins: 1 bin per pixel in space and say 5 bins in time. Feel free to write your own function to compute the 3D histogram or to use numpy's [histogramdd](https://numpy.org/doc/stable/reference/generated/numpy.histogramdd.html?highlight=histogramdd#numpy.histogramdd) function. Actually it is good to compute the histogram using both methods and making sure that both agree.\n",
    "\n",
    "### Regular histogram\n",
    "Write python code to compute the 3D histogram and display it as a voxel grid. \n",
    "Every voxel counts the number of events falling (i.e., binned) within it, regardless of polarity.\n",
    "Suggestion: [this sample code](https://matplotlib.org/3.1.1/gallery/mplot3d/voxels_rgb.html#sphx-glr-gallery-mplot3d-voxels-rgb-py)\n",
    "\n",
    "![voxel histogram](images/voxel_nn.png)\n",
    "\n",
    "- What are the minimum and maximum number of events at any voxel?\n",
    "- How many voxels are there? How many are \"occupied\" (i.e., have a non-zero value)? (voxel grids are also known as \"occupancy maps\" in Robotics). What is the ratio between these numbers? (How sparse is the data?)\n",
    "\n",
    "### Interpolated histogram\n",
    "(Smooth histogram that also includes polarity information).\n",
    "Next, modify the code you have written to include polarity. That is, instead of counting events on every bin, \"count\" the polarity. Moreover, to make the histogram smoother, use a linear voting scheme by which an event splits its polarity in its two closest temporal bins. The split takes into account the distance from the event to both bins. (This idea of smoothing a histogram is similar to the idea of kernel density estimation). The following plot illustrates this idea of \"smoothing\" the histogram.\n",
    "\n",
    "![histogram smoothing](images/histogram_smoothing.png)\n",
    "\n",
    "The next figure shows the voxel grid obtained by using linear voting of polarity (per-voxel balance of linearly-interpolated polarity). Grayscale values are used: dark values represent negative events (voxels with negative balance of polarities) and bright values represent positive events (voxels with positive balance of polarities).\n",
    "\n",
    "![voxel linear voting with polarity](images/voxel_linvote_pol.png)\n",
    "\n",
    "- What are the minimum and maximum value of the balance of event polarities at the voxels?\n",
    "- Does this way of computing the histogram, including polarity, affect the above \"sparsity ratio\"?\n",
    "- How does this (3D) voxel grid representation compare to 2D representations? For example in terms of memory, in terms of preservation of the information contained in the event data?\n",
    "- What could the voxel grid representation be good for?\n",
    "- How would you interpret the above voxel grid in terms of a continuous \"polarity field\" $p(x,y,t)$ or the temporal derivative of the brightness signal arriving at the sensor $\\frac{\\partial L}{\\partial t}(x,y,t)$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try experimenting\n",
    "Try to come up with new plots (alternative data representations) that reveal some aspect of the information contained in the event data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
