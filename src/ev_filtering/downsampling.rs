//! Polars-first event downsampling operations for reducing event density
//!
//! This module provides various strategies for reducing the number of events
//! while preserving important information, using Polars DataFrames and LazyFrames
//! for maximum performance and memory efficiency. All operations work directly with
//! Polars expressions and avoid manual Vec<Event> iterations.
//!
//! # Philosophy
//!
//! This implementation follows a strict Polars-first approach:
//! - Input: LazyFrame (from events_to_dataframe)
//! - Processing: Polars expressions and transformations
//! - Output: LazyFrame (convertible to Vec<Event> only when needed)
//!
//! # Performance Benefits
//!
//! - Lazy evaluation: Operations are optimized and executed only when needed
//! - Vectorized operations: All downsampling uses SIMD-optimized Polars operations
//! - Memory efficiency: No intermediate Vec<Event> allocations
//! - Query optimization: Polars optimizes the entire downsampling pipeline

use crate::ev_core::{Event, Events};
use crate::ev_filtering::config::Validatable;
use crate::ev_filtering::{FilterError, FilterResult, SingleFilter};
use polars::prelude::*;
use std::time::Instant;
use tracing::{debug, info, instrument, warn};

/// Polars column names for event data
pub const COL_X: &str = "x";
pub const COL_Y: &str = "y";
pub const COL_T: &str = "t";
pub const COL_POLARITY: &str = "polarity";

/// Downsampling strategies
#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub enum DownsamplingStrategy {
    /// Uniform random sampling - keep each event with fixed probability
    Uniform,
    /// Temporal decimation - keep every Nth event in time
    TemporalDecimation,
    /// Spatial decimation - keep events from every Nth pixel
    SpatialDecimation,
    /// Adaptive sampling based on local event density
    Adaptive,
    /// Importance-based sampling - prioritize events with high information content
    ImportanceBased,
    /// Fixed count - keep exactly N events
    FixedCount,
}

impl DownsamplingStrategy {
    /// Get description of this strategy
    pub fn description(&self) -> &'static str {
        match self {
            DownsamplingStrategy::Uniform => "uniform random",
            DownsamplingStrategy::TemporalDecimation => "temporal decimation",
            DownsamplingStrategy::SpatialDecimation => "spatial decimation",
            DownsamplingStrategy::Adaptive => "adaptive density",
            DownsamplingStrategy::ImportanceBased => "importance-based",
            DownsamplingStrategy::FixedCount => "fixed count",
        }
    }
}

/// Downsampling filter configuration
#[derive(Debug, Clone)]
pub struct DownsamplingFilter {
    /// Downsampling strategy to use
    pub strategy: DownsamplingStrategy,
    /// Sampling rate (0.0 to 1.0) for uniform and adaptive strategies
    pub sampling_rate: Option<f64>,
    /// Decimation factor (keep every Nth event)
    pub decimation_factor: Option<usize>,
    /// Target number of events for fixed count strategy
    pub target_count: Option<usize>,
    /// Time window for adaptive sampling (microseconds)
    pub adaptive_window_us: Option<f64>,
    /// Spatial window size for adaptive sampling (pixels)
    pub adaptive_spatial_window: Option<u16>,
    /// Whether to preserve temporal order
    pub preserve_order: bool,
    /// Random seed for reproducible sampling
    pub random_seed: Option<u64>,
    /// Whether to balance polarities in downsampling
    pub balance_polarities: bool,
}

impl DownsamplingFilter {
    /// Create uniform random downsampling filter
    ///
    /// # Arguments
    /// * `rate` - Sampling rate (0.0 to 1.0)
    pub fn uniform(rate: f64) -> Self {
        Self {
            strategy: DownsamplingStrategy::Uniform,
            sampling_rate: Some(rate),
            decimation_factor: None,
            target_count: None,
            adaptive_window_us: None,
            adaptive_spatial_window: None,
            preserve_order: true,
            random_seed: None,
            balance_polarities: false,
        }
    }

    /// Create temporal decimation filter
    ///
    /// # Arguments
    /// * `factor` - Keep every Nth event
    pub fn temporal_decimation(factor: usize) -> Self {
        Self {
            strategy: DownsamplingStrategy::TemporalDecimation,
            sampling_rate: None,
            decimation_factor: Some(factor),
            target_count: None,
            adaptive_window_us: None,
            adaptive_spatial_window: None,
            preserve_order: true,
            random_seed: None,
            balance_polarities: false,
        }
    }

    /// Create spatial decimation filter
    ///
    /// # Arguments
    /// * `factor` - Spatial decimation factor
    pub fn spatial_decimation(factor: usize) -> Self {
        Self {
            strategy: DownsamplingStrategy::SpatialDecimation,
            sampling_rate: None,
            decimation_factor: Some(factor),
            target_count: None,
            adaptive_window_us: None,
            adaptive_spatial_window: None,
            preserve_order: true,
            random_seed: None,
            balance_polarities: false,
        }
    }

    /// Create adaptive downsampling filter
    ///
    /// # Arguments
    /// * `rate` - Base sampling rate
    /// * `window_us` - Time window for density calculation
    /// * `spatial_window` - Spatial window size
    pub fn adaptive(rate: f64, window_us: f64, spatial_window: u16) -> Self {
        Self {
            strategy: DownsamplingStrategy::Adaptive,
            sampling_rate: Some(rate),
            decimation_factor: None,
            target_count: None,
            adaptive_window_us: Some(window_us),
            adaptive_spatial_window: Some(spatial_window),
            preserve_order: true,
            random_seed: None,
            balance_polarities: false,
        }
    }

    /// Create importance-based downsampling filter
    ///
    /// # Arguments
    /// * `rate` - Base sampling rate
    pub fn importance_based(rate: f64) -> Self {
        Self {
            strategy: DownsamplingStrategy::ImportanceBased,
            sampling_rate: Some(rate),
            decimation_factor: None,
            target_count: None,
            adaptive_window_us: None,
            adaptive_spatial_window: None,
            preserve_order: true,
            random_seed: None,
            balance_polarities: false,
        }
    }

    /// Create fixed count downsampling filter
    ///
    /// # Arguments
    /// * `count` - Target number of events to keep
    pub fn fixed_count(count: usize) -> Self {
        Self {
            strategy: DownsamplingStrategy::FixedCount,
            sampling_rate: None,
            decimation_factor: None,
            target_count: Some(count),
            adaptive_window_us: None,
            adaptive_spatial_window: None,
            preserve_order: true,
            random_seed: None,
            balance_polarities: false,
        }
    }

    /// Set random seed for reproducible sampling
    pub fn with_seed(mut self, seed: u64) -> Self {
        self.random_seed = Some(seed);
        self
    }

    /// Set whether to preserve temporal order
    pub fn with_order_preservation(mut self, preserve: bool) -> Self {
        self.preserve_order = preserve;
        self
    }

    /// Set whether to balance polarities
    pub fn with_polarity_balance(mut self, balance: bool) -> Self {
        self.balance_polarities = balance;
        self
    }

    /// Get description of this filter
    pub fn description(&self) -> String {
        let mut parts = vec![self.strategy.description().to_string()];

        if let Some(rate) = self.sampling_rate {
            parts.push(format!("rate: {:.3}", rate));
        }

        if let Some(factor) = self.decimation_factor {
            parts.push(format!("factor: {}", factor));
        }

        if let Some(count) = self.target_count {
            parts.push(format!("target: {}", count));
        }

        if self.balance_polarities {
            parts.push("balanced polarities".to_string());
        }

        parts.join(", ")
    }
}

impl Default for DownsamplingFilter {
    fn default() -> Self {
        Self::uniform(0.5) // 50% sampling by default
    }
}

impl Validatable for DownsamplingFilter {
    fn validate(&self) -> FilterResult<()> {
        match self.strategy {
            DownsamplingStrategy::Uniform => {
                if let Some(rate) = self.sampling_rate {
                    if rate <= 0.0 || rate > 1.0 {
                        return Err(FilterError::InvalidConfig(format!(
                            "Uniform sampling rate must be between 0.0 and 1.0, got {}",
                            rate
                        )));
                    }
                } else {
                    return Err(FilterError::InvalidConfig(
                        "Uniform strategy requires sampling_rate".to_string(),
                    ));
                }
            }
            DownsamplingStrategy::TemporalDecimation | DownsamplingStrategy::SpatialDecimation => {
                if let Some(factor) = self.decimation_factor {
                    if factor == 0 {
                        return Err(FilterError::InvalidConfig(
                            "Decimation factor must be positive".to_string(),
                        ));
                    }
                } else {
                    return Err(FilterError::InvalidConfig(
                        "Decimation strategy requires decimation_factor".to_string(),
                    ));
                }
            }
            DownsamplingStrategy::Adaptive => {
                if self.sampling_rate.is_none() || self.adaptive_window_us.is_none() {
                    return Err(FilterError::InvalidConfig(
                        "Adaptive strategy requires sampling_rate and adaptive_window_us"
                            .to_string(),
                    ));
                }
                if let Some(window) = self.adaptive_window_us {
                    if window <= 0.0 {
                        return Err(FilterError::InvalidConfig(
                            "Adaptive window must be positive".to_string(),
                        ));
                    }
                }
            }
            DownsamplingStrategy::ImportanceBased => {
                if self.sampling_rate.is_none() {
                    return Err(FilterError::InvalidConfig(
                        "Importance-based strategy requires sampling_rate".to_string(),
                    ));
                }
            }
            DownsamplingStrategy::FixedCount => {
                if self.target_count.is_none() {
                    return Err(FilterError::InvalidConfig(
                        "Fixed count strategy requires target_count".to_string(),
                    ));
                }
            }
        }

        Ok(())
    }
}

impl SingleFilter for DownsamplingFilter {
    fn apply(&self, events: &Events) -> FilterResult<Events> {
        apply_downsampling_filter(events, self)
    }

    fn description(&self) -> String {
        format!("Downsampling filter: {}", self.description())
    }

    fn is_enabled(&self) -> bool {
        true
    }
}

//
// ============================================================================
// POLARS-FIRST IMPLEMENTATIONS
// ============================================================================
//

/// Apply downsampling filter to LazyFrame using Polars expressions
///
/// This is the primary Polars-first implementation that reduces the number of events
/// according to the specified downsampling strategy using efficient Polars operations.
///
/// # Arguments
///
/// * `df` - Input LazyFrame with event data
/// * `filter` - Downsampling filter configuration
///
/// # Returns
///
/// * `PolarsResult<LazyFrame>` - Downsampled events as LazyFrame
///
/// # Example
///
/// ```rust
/// use polars::prelude::*;
/// use evlib::ev_filtering::downsampling::*;
///
/// // Convert events to LazyFrame once
/// let events_df = events_to_dataframe(&events)?.lazy();
///
/// // Apply downsampling with Polars expressions
/// let downsampled = apply_downsampling_filter_polars(events_df, &DownsamplingFilter::uniform(0.5))?;
/// ```
#[instrument(skip(df), fields(strategy = ?filter.strategy))]
pub fn apply_downsampling_filter_polars(
    df: LazyFrame,
    filter: &DownsamplingFilter,
) -> PolarsResult<LazyFrame> {
    let start_time = Instant::now();

    // Validate filter configuration first
    if let Err(e) = filter.validate() {
        return Err(PolarsError::ComputeError(
            format!("Invalid downsampling configuration: {}", e).into(),
        ));
    }

    let downsampled_df = match filter.strategy {
        DownsamplingStrategy::Uniform => {
            apply_uniform_sampling_polars(df, filter.sampling_rate.unwrap(), filter.random_seed)?
        }
        DownsamplingStrategy::TemporalDecimation => {
            apply_temporal_decimation_polars(df, filter.decimation_factor.unwrap())?
        }
        DownsamplingStrategy::SpatialDecimation => {
            apply_spatial_decimation_polars(df, filter.decimation_factor.unwrap())?
        }
        DownsamplingStrategy::Adaptive => apply_adaptive_sampling_polars(
            df,
            filter.sampling_rate.unwrap(),
            filter.adaptive_window_us.unwrap(),
            filter.adaptive_spatial_window.unwrap_or(3),
        )?,
        DownsamplingStrategy::ImportanceBased => {
            apply_importance_based_sampling_polars(df, filter.sampling_rate.unwrap())?
        }
        DownsamplingStrategy::FixedCount => {
            apply_fixed_count_sampling_polars(df, filter.target_count.unwrap(), filter.random_seed)?
        }
    };

    let mut final_df = downsampled_df;

    // Balance polarities if requested
    if filter.balance_polarities {
        final_df = balance_polarity_sampling_polars(final_df)?;
    }

    // Preserve temporal order if requested
    if filter.preserve_order {
        final_df = final_df.sort([COL_T], SortMultipleOptions::default());
    }

    let processing_time = start_time.elapsed().as_secs_f64();
    info!(
        "Polars downsampling ({}): {:.3}s processing time",
        filter.strategy.description(),
        processing_time
    );

    Ok(final_df)
}

/// Apply uniform random sampling using Polars sample() function
///
/// This function uses Polars' optimized random sampling which is much more efficient
/// than manual iteration over events.
#[instrument(skip(df), fields(sampling_rate = sampling_rate))]
pub fn apply_uniform_sampling_polars(
    df: LazyFrame,
    sampling_rate: f64,
    seed: Option<u64>,
) -> PolarsResult<LazyFrame> {
    debug!(
        "Applying Polars uniform sampling with rate: {:.3}",
        sampling_rate
    );

    // Use Polars' sample function for efficient random sampling
    // Note: sample_frac not available on LazyFrame, so collect first
    let collected_df = df.collect()?;
    // Create Series for sampling_rate parameter
    let frac_series = Series::new("".into(), [sampling_rate]);
    Ok(collected_df
        .sample_frac(
            &frac_series,
            false, // with_replacement = false
            true,  // shuffle
            seed,
        )?
        .lazy())
}

/// Apply temporal decimation using Polars slice operations
///
/// This uses Polars' optimized slice operations instead of manual step_by iteration.
#[instrument(skip(df), fields(factor = factor))]
pub fn apply_temporal_decimation_polars(df: LazyFrame, factor: usize) -> PolarsResult<LazyFrame> {
    debug!(
        "Applying Polars temporal decimation with factor: {}",
        factor
    );

    // Use Polars expressions to select every Nth row efficiently
    Ok(df
        .with_row_index("__temp_idx", None)
        .filter((col("__temp_idx") % lit(factor as i64)).eq(lit(0)))
        .drop(["__temp_idx"]))
}

/// Apply spatial decimation using Polars filter expressions
///
/// This replaces the manual pixel coordinate checking loop with efficient Polars expressions.
#[instrument(skip(df), fields(factor = factor))]
pub fn apply_spatial_decimation_polars(df: LazyFrame, factor: usize) -> PolarsResult<LazyFrame> {
    debug!("Applying Polars spatial decimation with factor: {}", factor);

    // Use Polars expressions for efficient coordinate filtering
    Ok(df.filter(
        (col(COL_X) % lit(factor as i64))
            .eq(lit(0))
            .and((col(COL_Y) % lit(factor as i64)).eq(lit(0))),
    ))
}

/// Apply fixed count sampling using Polars sample() function
///
/// This replaces the reservoir sampling algorithm with Polars' optimized sample function.
#[instrument(skip(df), fields(target_count = target_count))]
pub fn apply_fixed_count_sampling_polars(
    df: LazyFrame,
    target_count: usize,
    seed: Option<u64>,
) -> PolarsResult<LazyFrame> {
    debug!(
        "Applying Polars fixed count sampling with target: {}",
        target_count
    );

    // Use Polars' sample function for efficient fixed-count sampling
    // Note: sample_n not available on LazyFrame, so collect first
    let collected_df = df.collect()?;
    // Create Series for target_count parameter
    let n_series = Series::new("".into(), [target_count as u32]);
    Ok(collected_df
        .sample_n(
            &n_series, false, // with_replacement = false
            true,  // shuffle
            seed,
        )?
        .lazy())
}

/// Balance polarity distribution using Polars group_by operations
///
/// This replaces the manual polarity separation and balancing with efficient Polars operations.
#[instrument(skip(df))]
pub fn balance_polarity_sampling_polars(df: LazyFrame) -> PolarsResult<LazyFrame> {
    debug!("Applying Polars polarity balancing");

    // Calculate minimum count per polarity using group_by
    let min_count_df = df
        .clone()
        .group_by([col(COL_POLARITY)])
        .agg([len().alias("count")])
        .select([col("count").min().alias("min_count")])
        .collect()?;

    if min_count_df.height() == 0 {
        return Ok(df);
    }

    let min_count = min_count_df.column("min_count")?.u32()?.get(0).unwrap_or(0) as usize;

    if min_count == 0 {
        return Ok(df.limit(0)); // Return empty if no events of one polarity
    }

    // Sample equal numbers from each polarity
    // Note: sample_n not available on LazyFrame, so collect first
    let positive_collected = df.clone().filter(col(COL_POLARITY).gt(lit(0))).collect()?;
    // Create Series for min_count parameter
    let min_count_series = Series::new("".into(), [min_count as u32]);
    let positive_sample = positive_collected
        .sample_n(
            &min_count_series,
            false, // with_replacement = false
            false, // shuffle
            None,  // seed
        )?
        .lazy();

    let negative_collected = df.filter(col(COL_POLARITY).eq(lit(0))).collect()?;
    let negative_sample = negative_collected
        .sample_n(
            &min_count_series,
            false, // with_replacement = false
            false, // shuffle
            None,  // seed
        )?
        .lazy();

    // Combine the balanced samples
    concat([positive_sample, negative_sample], Default::default())
}

/// Apply adaptive sampling using Polars window functions
///
/// This replaces the nested loops with efficient Polars window operations for
/// calculating local event density.
#[instrument(skip(df), fields(base_rate = base_rate, window_us = window_us, spatial_window = spatial_window))]
pub fn apply_adaptive_sampling_polars(
    df: LazyFrame,
    base_rate: f64,
    window_us: f64,
    spatial_window: u16,
) -> PolarsResult<LazyFrame> {
    debug!(
        "Applying Polars adaptive sampling: rate={:.3}, window={}Î¼s, spatial={}px",
        base_rate, window_us, spatial_window
    );

    let _window_sec = window_us / 1_000_000.0;

    // Add window-based density calculations using Polars expressions
    let _df_with_density = df
        .clone()
        .sort([COL_T], SortMultipleOptions::default())
        .with_columns([
            // Calculate local temporal density using rolling window
            // TODO: Implement proper rolling operations for temporal density
            lit(1).alias("temporal_density"), // Placeholder for now
            // Add random values for sampling decisions
            when(lit(true))
                .then(lit(0.0)) // Will be replaced with random values
                .otherwise(lit(1.0))
                .alias("random_val"),
        ])
        .with_columns([
            // Calculate adaptive sampling rate based on density
            // Simplified adaptive rate calculation (Polars 0.49.1 compatible)
            when(col("temporal_density").gt(lit(1.0)))
                .then(
                    when(
                        (lit(base_rate) / col("temporal_density").cast(DataType::Float64))
                            .lt(lit(0.01)),
                    )
                    .then(lit(0.01))
                    .otherwise(
                        when(
                            (lit(base_rate) / col("temporal_density").cast(DataType::Float64))
                                .gt(lit(1.0)),
                        )
                        .then(lit(1.0))
                        .otherwise(
                            lit(base_rate) / col("temporal_density").cast(DataType::Float64),
                        ),
                    ),
                )
                .otherwise(lit(base_rate))
                .alias("adaptive_rate"),
        ]);

    // For now, fall back to uniform sampling with base rate
    // TODO: Implement proper density-based adaptive sampling with spatial windows
    warn!("Adaptive sampling using simplified uniform rate - full spatial-temporal density calculation not yet implemented in Polars");
    apply_uniform_sampling_polars(df, base_rate, None)
}

/// Apply importance-based sampling using Polars expressions for scoring
///
/// This replaces the manual importance calculation with Polars expressions.
#[instrument(skip(df), fields(base_rate = base_rate))]
pub fn apply_importance_based_sampling_polars(
    df: LazyFrame,
    base_rate: f64,
) -> PolarsResult<LazyFrame> {
    debug!(
        "Applying Polars importance-based sampling with rate: {:.3}",
        base_rate
    );

    let _df_with_importance = df
        .clone()
        .sort([COL_T], SortMultipleOptions::default())
        .with_columns([
            // Calculate importance scores using Polars expressions
            // Factor 1: Polarity changes increase importance
            col(COL_POLARITY)
                .neq(col(COL_POLARITY).shift(lit(1)))
                .fill_null(lit(false))
                .cast(DataType::Float64)
                * lit(0.5)
                + lit(1.0).alias("polarity_importance"),
            // Factor 2: Time difference patterns
            (col(COL_T) - col(COL_T).shift(lit(1)))
                .fill_null(lit(0.001))
                .alias("time_diff"),
        ])
        .with_columns([
            // Factor 3: Unusual timing patterns
            when(
                col("time_diff")
                    .lt(lit(0.0001))
                    .or(col("time_diff").gt(lit(0.01))),
            )
            .then(lit(1.2))
            .otherwise(lit(1.0))
            .alias("timing_importance"),
        ])
        .with_columns([
            // Combined importance score
            (col("polarity_importance") * col("timing_importance")).alias("importance_score"),
        ])
        .with_columns([
            // Sampling probability based on importance
            // Clamp sampling probability between 0.0 and 1.0 (Polars 0.49.1 compatible)
            when((lit(base_rate) * col("importance_score")).lt(lit(0.0)))
                .then(lit(0.0))
                .otherwise(
                    when((lit(base_rate) * col("importance_score")).gt(lit(1.0)))
                        .then(lit(1.0))
                        .otherwise(lit(base_rate) * col("importance_score")),
                )
                .alias("sampling_prob"),
        ]);

    // For now, use uniform sampling with base rate
    // TODO: Implement proper importance-weighted sampling
    warn!("Importance-based sampling using simplified uniform rate - full importance weighting not yet implemented in Polars");
    apply_uniform_sampling_polars(df, base_rate, None)
}

//
// ============================================================================
// CONVENIENCE FUNCTIONS FOR POLARS-FIRST USAGE
// ============================================================================
//

/// Convenience function for Polars uniform downsampling
///
/// # Example
///
/// ```rust
/// use polars::prelude::*;
/// use evlib::ev_filtering::downsampling::*;
///
/// let events_df = events_to_dataframe(&events)?.lazy();
/// let downsampled = downsample_uniform_polars(events_df, 0.5)?;
/// ```
pub fn downsample_uniform_polars(df: LazyFrame, rate: f64) -> PolarsResult<LazyFrame> {
    let filter = DownsamplingFilter::uniform(rate);
    apply_downsampling_filter_polars(df, &filter)
}

/// Convenience function for Polars event count reduction
pub fn downsample_events_polars(df: LazyFrame, target_count: usize) -> PolarsResult<LazyFrame> {
    let filter = DownsamplingFilter::fixed_count(target_count);
    apply_downsampling_filter_polars(df, &filter)
}

/// Convenience function for Polars temporal decimation
pub fn downsample_temporal_decimation_polars(
    df: LazyFrame,
    factor: usize,
) -> PolarsResult<LazyFrame> {
    let filter = DownsamplingFilter::temporal_decimation(factor);
    apply_downsampling_filter_polars(df, &filter)
}

/// Convenience function for Polars spatial decimation
pub fn downsample_spatial_decimation_polars(
    df: LazyFrame,
    factor: usize,
) -> PolarsResult<LazyFrame> {
    let filter = DownsamplingFilter::spatial_decimation(factor);
    apply_downsampling_filter_polars(df, &filter)
}

//
// ============================================================================
// LEGACY VEC<EVENT> IMPLEMENTATIONS - DELEGATE TO POLARS
// ============================================================================
//

/// Apply downsampling filter to events (LEGACY - delegates to Polars)
///
/// This function reduces the number of events according to the specified
/// downsampling strategy. This is a legacy interface that delegates to the
/// Polars-first implementation for better performance.
///
/// # Arguments
///
/// * `events` - Input events to downsample
/// * `filter` - Downsampling filter configuration
///
/// # Returns
///
/// * `FilterResult<Events>` - Downsampled events
pub fn apply_downsampling_filter(
    events: &Events,
    filter: &DownsamplingFilter,
) -> FilterResult<Events> {
    let start_time = std::time::Instant::now();

    if events.is_empty() {
        debug!("No events to downsample");
        return Ok(Vec::new());
    }

    warn!("Using legacy Vec<Event> interface for downsampling - consider using LazyFrame directly");

    // Convert to LazyFrame, apply Polars filter, convert back
    let df = crate::ev_core::events_to_dataframe(events)
        .map_err(|e| {
            FilterError::ProcessingError(format!("Failed to convert events to DataFrame: {}", e))
        })?
        .lazy();

    let downsampled_df = apply_downsampling_filter_polars(df, filter)
        .map_err(|e| FilterError::ProcessingError(format!("Polars downsampling failed: {}", e)))?;

    let result_df = downsampled_df
        .collect()
        .map_err(|e| FilterError::ProcessingError(format!("Failed to collect LazyFrame: {}", e)))?;

    let downsampled_events = dataframe_to_events(&result_df)?;

    let processing_time = start_time.elapsed().as_secs_f64();
    let input_count = events.len();
    let output_count = downsampled_events.len();
    let reduction_ratio = 1.0 - (output_count as f64 / input_count as f64);

    info!(
        "Legacy downsampling ({}): {} -> {} events ({:.1}% reduction) in {:.3}s",
        filter.strategy.description(),
        input_count,
        output_count,
        reduction_ratio * 100.0,
        processing_time
    );

    Ok(downsampled_events)
}

/// Apply uniform random sampling (LEGACY - delegates to Polars)
#[allow(dead_code)]
fn apply_uniform_sampling(
    events: &Events,
    sampling_rate: f64,
    seed: Option<u64>,
) -> FilterResult<Events> {
    warn!("Using legacy uniform sampling - consider using apply_uniform_sampling_polars directly");

    let df = crate::ev_core::events_to_dataframe(events)
        .map_err(|e| FilterError::ProcessingError(format!("DataFrame conversion failed: {}", e)))?
        .lazy();

    let sampled_df = apply_uniform_sampling_polars(df, sampling_rate, seed).map_err(|e| {
        FilterError::ProcessingError(format!("Polars uniform sampling failed: {}", e))
    })?;

    let result_df = sampled_df
        .collect()
        .map_err(|e| FilterError::ProcessingError(format!("Failed to collect LazyFrame: {}", e)))?;

    dataframe_to_events(&result_df)
}

/// Apply temporal decimation (LEGACY - delegates to Polars)
#[allow(dead_code)]
fn apply_temporal_decimation(events: &Events, factor: usize) -> FilterResult<Events> {
    warn!("Using legacy temporal decimation - consider using apply_temporal_decimation_polars directly");

    let df = crate::ev_core::events_to_dataframe(events)
        .map_err(|e| FilterError::ProcessingError(format!("DataFrame conversion failed: {}", e)))?
        .lazy();

    let decimated_df = apply_temporal_decimation_polars(df, factor).map_err(|e| {
        FilterError::ProcessingError(format!("Polars temporal decimation failed: {}", e))
    })?;

    let result_df = decimated_df
        .collect()
        .map_err(|e| FilterError::ProcessingError(format!("Failed to collect LazyFrame: {}", e)))?;

    dataframe_to_events(&result_df)
}

/// Apply spatial decimation (LEGACY - delegates to Polars)
#[allow(dead_code)]
fn apply_spatial_decimation(events: &Events, factor: usize) -> FilterResult<Events> {
    warn!(
        "Using legacy spatial decimation - consider using apply_spatial_decimation_polars directly"
    );

    let df = crate::ev_core::events_to_dataframe(events)
        .map_err(|e| FilterError::ProcessingError(format!("DataFrame conversion failed: {}", e)))?
        .lazy();

    let decimated_df = apply_spatial_decimation_polars(df, factor).map_err(|e| {
        FilterError::ProcessingError(format!("Polars spatial decimation failed: {}", e))
    })?;

    let result_df = decimated_df
        .collect()
        .map_err(|e| FilterError::ProcessingError(format!("Failed to collect LazyFrame: {}", e)))?;

    dataframe_to_events(&result_df)
}

/// Apply adaptive sampling based on local event density (LEGACY - delegates to Polars)
#[allow(dead_code)]
fn apply_adaptive_sampling(
    events: &Events,
    base_rate: f64,
    window_us: f64,
    spatial_window: u16,
) -> FilterResult<Events> {
    warn!(
        "Using legacy adaptive sampling - consider using apply_adaptive_sampling_polars directly"
    );

    let df = crate::ev_core::events_to_dataframe(events)
        .map_err(|e| FilterError::ProcessingError(format!("DataFrame conversion failed: {}", e)))?
        .lazy();

    let adaptive_df = apply_adaptive_sampling_polars(df, base_rate, window_us, spatial_window)
        .map_err(|e| {
            FilterError::ProcessingError(format!("Polars adaptive sampling failed: {}", e))
        })?;

    let result_df = adaptive_df
        .collect()
        .map_err(|e| FilterError::ProcessingError(format!("Failed to collect LazyFrame: {}", e)))?;

    dataframe_to_events(&result_df)
}

/// Apply importance-based sampling (LEGACY - delegates to Polars)
#[allow(dead_code)]
fn apply_importance_based_sampling(events: &Events, base_rate: f64) -> FilterResult<Events> {
    warn!("Using legacy importance-based sampling - consider using apply_importance_based_sampling_polars directly");

    let df = crate::ev_core::events_to_dataframe(events)
        .map_err(|e| FilterError::ProcessingError(format!("DataFrame conversion failed: {}", e)))?
        .lazy();

    let importance_df = apply_importance_based_sampling_polars(df, base_rate).map_err(|e| {
        FilterError::ProcessingError(format!("Polars importance-based sampling failed: {}", e))
    })?;

    let result_df = importance_df
        .collect()
        .map_err(|e| FilterError::ProcessingError(format!("Failed to collect LazyFrame: {}", e)))?;

    dataframe_to_events(&result_df)
}

/// Apply fixed count sampling (LEGACY - delegates to Polars)
#[allow(dead_code)]
fn apply_fixed_count_sampling(
    events: &Events,
    target_count: usize,
    seed: Option<u64>,
) -> FilterResult<Events> {
    warn!("Using legacy fixed count sampling - consider using apply_fixed_count_sampling_polars directly");

    let df = crate::ev_core::events_to_dataframe(events)
        .map_err(|e| FilterError::ProcessingError(format!("DataFrame conversion failed: {}", e)))?
        .lazy();

    let sampled_df = apply_fixed_count_sampling_polars(df, target_count, seed).map_err(|e| {
        FilterError::ProcessingError(format!("Polars fixed count sampling failed: {}", e))
    })?;

    let result_df = sampled_df
        .collect()
        .map_err(|e| FilterError::ProcessingError(format!("Failed to collect LazyFrame: {}", e)))?;

    dataframe_to_events(&result_df)
}

/// Balance polarity distribution in sampled events (LEGACY - delegates to Polars)
#[allow(dead_code)]
fn balance_polarity_sampling(events: &Events) -> FilterResult<Events> {
    warn!("Using legacy polarity balancing - consider using balance_polarity_sampling_polars directly");

    let df = crate::ev_core::events_to_dataframe(events)
        .map_err(|e| FilterError::ProcessingError(format!("DataFrame conversion failed: {}", e)))?
        .lazy();

    let balanced_df = balance_polarity_sampling_polars(df).map_err(|e| {
        FilterError::ProcessingError(format!("Polars polarity balancing failed: {}", e))
    })?;

    let result_df = balanced_df
        .collect()
        .map_err(|e| FilterError::ProcessingError(format!("Failed to collect LazyFrame: {}", e)))?;

    dataframe_to_events(&result_df)
}

/// Convenience function for uniform downsampling (LEGACY - delegates to Polars)
pub fn downsample_uniform(events: &Events, rate: f64) -> FilterResult<Events> {
    warn!("Using legacy uniform downsampling convenience function - consider using downsample_uniform_polars");
    let filter = DownsamplingFilter::uniform(rate);
    apply_downsampling_filter(events, &filter)
}

/// Convenience function for event count reduction (LEGACY - delegates to Polars)
pub fn downsample_events(events: &Events, target_count: usize) -> FilterResult<Events> {
    warn!("Using legacy event count reduction convenience function - consider using downsample_events_polars");
    let filter = DownsamplingFilter::fixed_count(target_count);
    apply_downsampling_filter(events, &filter)
}

/// Convert DataFrame back to Events vector (for compatibility with legacy interfaces)
fn dataframe_to_events(df: &DataFrame) -> FilterResult<Events> {
    let height = df.height();
    let mut events = Vec::with_capacity(height);

    let x_series = df
        .column(COL_X)
        .map_err(|e| FilterError::ProcessingError(format!("Missing x column: {}", e)))?
        .u16()
        .map_err(|e| FilterError::ProcessingError(format!("Invalid x column type: {}", e)))?;

    let y_series = df
        .column(COL_Y)
        .map_err(|e| FilterError::ProcessingError(format!("Missing y column: {}", e)))?
        .u16()
        .map_err(|e| FilterError::ProcessingError(format!("Invalid y column type: {}", e)))?;

    let t_series = df
        .column(COL_T)
        .map_err(|e| FilterError::ProcessingError(format!("Missing t column: {}", e)))?
        .f64()
        .map_err(|e| FilterError::ProcessingError(format!("Invalid t column type: {}", e)))?;

    let polarity_series = df
        .column(COL_POLARITY)
        .map_err(|e| FilterError::ProcessingError(format!("Missing polarity column: {}", e)))?
        .bool()
        .map_err(|e| {
            FilterError::ProcessingError(format!("Invalid polarity column type: {}", e))
        })?;

    for i in 0..height {
        let x = x_series.get(i).ok_or_else(|| {
            FilterError::ProcessingError(format!("Missing x value at index {}", i))
        })?;
        let y = y_series.get(i).ok_or_else(|| {
            FilterError::ProcessingError(format!("Missing y value at index {}", i))
        })?;
        let t = t_series.get(i).ok_or_else(|| {
            FilterError::ProcessingError(format!("Missing t value at index {}", i))
        })?;
        let polarity = polarity_series.get(i).ok_or_else(|| {
            FilterError::ProcessingError(format!("Missing polarity value at index {}", i))
        })?;

        events.push(Event { x, y, t, polarity });
    }

    Ok(events)
}

/// Calculate optimal downsampling rate for target event count
pub fn calculate_optimal_rate(current_count: usize, target_count: usize) -> f64 {
    if current_count == 0 || target_count >= current_count {
        return 1.0;
    }

    (target_count as f64 / current_count as f64).min(1.0)
}

/// Estimate memory savings from downsampling
pub fn estimate_memory_savings(original_count: usize, downsampled_count: usize) -> (usize, f64) {
    let event_size = std::mem::size_of::<Event>();
    let original_memory = original_count * event_size;
    let downsampled_memory = downsampled_count * event_size;
    let savings = original_memory.saturating_sub(downsampled_memory);
    let savings_percentage = if original_memory > 0 {
        (savings as f64 / original_memory as f64) * 100.0
    } else {
        0.0
    };

    (savings, savings_percentage)
}

#[cfg(test)]
mod tests {
    use super::*;
    use crate::ev_core::Event;

    fn create_test_events() -> Events {
        let mut events = Vec::new();

        // Create events with varying patterns
        for i in 0..100 {
            events.push(Event {
                t: i as f64 * 0.001, // 1ms intervals
                x: (i % 10) as u16 * 10,
                y: (i / 10) as u16 * 10,
                polarity: i % 2 == 0,
            });
        }

        events
    }

    #[test]
    fn test_downsampling_filter_creation() {
        let filter = DownsamplingFilter::uniform(0.5);
        assert_eq!(filter.strategy, DownsamplingStrategy::Uniform);
        assert_eq!(filter.sampling_rate, Some(0.5));

        let filter = DownsamplingFilter::temporal_decimation(3);
        assert_eq!(filter.strategy, DownsamplingStrategy::TemporalDecimation);
        assert_eq!(filter.decimation_factor, Some(3));

        let filter = DownsamplingFilter::fixed_count(50);
        assert_eq!(filter.strategy, DownsamplingStrategy::FixedCount);
        assert_eq!(filter.target_count, Some(50));
    }

    #[test]
    fn test_uniform_downsampling() {
        let events = create_test_events();
        let original_count = events.len();

        // Use a fixed seed for reproducible results
        let filter = DownsamplingFilter::uniform(0.5).with_seed(12345);
        let downsampled = apply_downsampling_filter(&events, &filter).unwrap();

        // Should approximately halve the events (with some randomness)
        assert!(downsampled.len() < original_count);
        assert!(downsampled.len() > original_count / 4); // At least 25%
        assert!(downsampled.len() < original_count * 3 / 4); // At most 75%
    }

    #[test]
    fn test_temporal_decimation() {
        let events = create_test_events();
        let downsampled = downsample_temporal_decimation(&events, 3);

        // Should keep every 3rd event
        assert_eq!(downsampled.len(), events.len() / 3);

        // Check that selected events are spaced correctly
        for i in 0..downsampled.len() {
            assert_eq!(downsampled[i], events[i * 3]);
        }
    }

    #[test]
    fn test_spatial_decimation() {
        let events = create_test_events();
        let filter = DownsamplingFilter::spatial_decimation(2);
        let downsampled = apply_downsampling_filter(&events, &filter).unwrap();

        // Should only keep events from even coordinates
        for event in &downsampled {
            assert_eq!(event.x % 2, 0);
            assert_eq!(event.y % 2, 0);
        }
    }

    #[test]
    fn test_fixed_count_downsampling() {
        let events = create_test_events();
        let target_count = 30;

        let downsampled = downsample_events(&events, target_count).unwrap();

        assert_eq!(downsampled.len(), target_count);
    }

    #[test]
    fn test_adaptive_downsampling() {
        let events = create_test_events();
        let filter = DownsamplingFilter::adaptive(0.5, 5000.0, 3);
        let downsampled = apply_downsampling_filter(&events, &filter).unwrap();

        // Should reduce events, but exact count depends on local density
        assert!(downsampled.len() <= events.len());
        assert!(downsampled.len() > 0);
    }

    #[test]
    fn test_importance_based_downsampling() {
        let events = create_test_events();
        let filter = DownsamplingFilter::importance_based(0.3);
        let downsampled = apply_downsampling_filter(&events, &filter).unwrap();

        // Should select events based on importance
        assert!(downsampled.len() <= events.len());
        assert!(downsampled.len() > 0);
    }

    #[test]
    fn test_polarity_balance() {
        let mut events = Vec::new();

        // Create unbalanced events (more positive than negative)
        for i in 0..80 {
            events.push(Event {
                t: i as f64,
                x: 100,
                y: 200,
                polarity: true, // All positive
            });
        }
        for i in 0..20 {
            events.push(Event {
                t: (i + 80) as f64,
                x: 100,
                y: 200,
                polarity: false, // Few negative
            });
        }

        let filter = DownsamplingFilter::uniform(1.0).with_polarity_balance(true);
        let balanced = apply_downsampling_filter(&events, &filter).unwrap();

        // Should balance polarities
        let positive_count = balanced.iter().filter(|e| e.polarity).count();
        let negative_count = balanced.iter().filter(|e| !e.polarity).count();
        assert_eq!(positive_count, negative_count);
        assert_eq!(balanced.len(), 40); // 20 of each polarity
    }

    #[test]
    fn test_filter_validation() {
        // Valid filters
        assert!(DownsamplingFilter::uniform(0.5).validate().is_ok());
        assert!(DownsamplingFilter::temporal_decimation(3)
            .validate()
            .is_ok());
        assert!(DownsamplingFilter::fixed_count(100).validate().is_ok());

        // Invalid filters
        let mut invalid_uniform = DownsamplingFilter::uniform(0.5);
        invalid_uniform.sampling_rate = Some(1.5); // > 1.0
        assert!(invalid_uniform.validate().is_err());

        let mut invalid_decimation = DownsamplingFilter::temporal_decimation(3);
        invalid_decimation.decimation_factor = Some(0); // Zero
        assert!(invalid_decimation.validate().is_err());
    }

    #[test]
    fn test_order_preservation() {
        let mut events = create_test_events();
        // Shuffle events to test order preservation
        events.swap(10, 50);
        events.swap(20, 80);

        let filter = DownsamplingFilter::uniform(0.8).with_order_preservation(true);
        let downsampled = apply_downsampling_filter(&events, &filter).unwrap();

        // Should be sorted after downsampling
        assert!(utils::is_sorted_by_time(&downsampled));
    }

    #[test]
    fn test_empty_events() {
        let events = Vec::new();
        let filter = DownsamplingFilter::uniform(0.5);
        let downsampled = apply_downsampling_filter(&events, &filter).unwrap();
        assert!(downsampled.is_empty());
    }

    #[test]
    fn test_convenience_functions() {
        let events = create_test_events();

        let uniform_downsampled = downsample_uniform(&events, 0.3).unwrap();
        assert!(uniform_downsampled.len() <= events.len());

        let count_downsampled = downsample_events(&events, 25).unwrap();
        assert_eq!(count_downsampled.len(), 25);
    }

    #[test]
    fn test_optimal_rate_calculation() {
        assert_eq!(calculate_optimal_rate(100, 50), 0.5);
        assert_eq!(calculate_optimal_rate(100, 100), 1.0);
        assert_eq!(calculate_optimal_rate(100, 150), 1.0); // Can't increase events
        assert_eq!(calculate_optimal_rate(0, 10), 1.0); // Handle zero case
    }

    #[test]
    fn test_memory_savings_estimation() {
        let (savings, percentage) = estimate_memory_savings(1000, 500);
        let expected_savings = 500 * std::mem::size_of::<Event>();

        assert_eq!(savings, expected_savings);
        assert!((percentage - 50.0).abs() < 0.1); // Approximately 50%
    }

    #[test]
    fn test_reproducible_sampling() {
        let events = create_test_events();
        let seed = 42;

        let filter1 = DownsamplingFilter::uniform(0.5).with_seed(seed);
        let downsampled1 = apply_downsampling_filter(&events, &filter1).unwrap();

        let filter2 = DownsamplingFilter::uniform(0.5).with_seed(seed);
        let downsampled2 = apply_downsampling_filter(&events, &filter2).unwrap();

        // Same seed should produce identical results
        assert_eq!(downsampled1.len(), downsampled2.len());
        for (e1, e2) in downsampled1.iter().zip(downsampled2.iter()) {
            assert_eq!(e1, e2);
        }
    }

    // Helper function for temporal decimation test
    fn downsample_temporal_decimation(events: &Events, factor: usize) -> Events {
        events.iter().step_by(factor).cloned().collect()
    }
}
