I ultimately want evlib to be the one stop shop for event based vision utilities
as well as a place where many of the state of the art algorithms have been
implemented and reside.

Events to Video algorithms (vid2e) I would like to implement are:

- [E2VID](https://github.com/uzh-rpg/rpg_e2vid)
- [FireNet](https://github.com/cedric-scheerlinck/rpg_e2vid/tree/cedric/firenet)
- [E2VID+](https://github.com/TimoStoff/event_cnn_minimal)
- [FireNet+](https://github.com/TimoStoff/event_cnn_minimal)
- [SPADE-E2VID](https://github.com/RodrigoGantier/SPADE_E2VID)
- [SSL-E2VID](https://github.com/tudelft/ssl_e2vid)
- [ET-Net](https://github.com/WarranWeng/ET-Net)
- [HyperE2VID](https://github.com/ercanburak/HyperE2VID)

They all should use Candle framework to re-implement the architecture and use
existing pytorch model files where possible. We can leverage either the existing
PyTorch loader (pytorch_loader.rs) or ONNX Runtime (ort) for model inference.

https://github.com/ercanburak/EVREAL does a good job of comparing the different
algorithms and points to many good resources. I would like all of these
algorithms to have their implementation at: `evlib/src/ev_processing/reconstruction/`

After that my plan is to implement the functionality found here that does the
reverse, i.e Video to Events and works as a simulator. The main repo for this
can be found here: https://github.com/uzh-rpg/rpg_e2vid

A useful set of open-source tooling can be found here:
https://gitlab.com/inivation/dv/dv-processing. I would like to eventually
incorporate the functionality there inside evlib too.

A project that is aiming to do similar things is
https://github.com/ucsd-hdsi-dvs/V2CE-Toolbox, maybe you can draw inspiration
from how they do things.

Of course there is also https://github.com/prophesee-ai/openeb too and
https://github.com/shiba24/event-vision-library and actually there is a good
amount of tooling found here too: https://github.com/tub-rip/ETAP

## Implementation Status

### Recent Progress (January 2025)

- âœ… Implemented E2VID UNet and FireNet architectures in Candle
- âœ… Integrated ONNX Runtime (ort v2.0.0-rc.9) for model inference
- âœ… Added Python API with model selection (unet/firenet/onnx/simple)
- âœ… Comprehensive test suite for both Rust and Python APIs
- âœ… Successfully tested with slider_depth dataset
- âœ… PyTorch to ONNX model converter with validation
- âœ… GPU optimization utilities (CUDA/Metal support)
- âœ… **Phase 6 Complete**: Unified model loading system with multi-format support (.pth, .onnx, .safetensors)
- âœ… Model verification framework for cross-format validation
- âœ… Automatic format detection and priority-based loading
- âœ… EVREAL benchmarking metrics (MSE, PSNR, SSIM, MS-SSIM)
- âœ… Temporal consistency metrics for video sequences
- âœ… ConvLSTM implementation for temporal processing
- âœ… E2VID+ and FireNet+ architectures with temporal memory
- âœ… SPADE normalization layers and SPADE-E2VID variants
- âœ… SSL-E2VID with self-supervised learning framework
- âœ… Python bindings for SPADE and SSL models

## Implementation Plan

### Phase 1: Foundation (4-6 weeks) - âœ… COMPLETED

**Priority: HIGH**

1. **Core Infrastructure** âœ… COMPLETED

   - âœ… ONNX Runtime (ort) integration
   - âœ… Enhanced voxel grid representations
   - âœ… Candle-based E2VID UNet and FireNet architectures
   - âœ… PyTorch to ONNX model converter
   - âœ… GPU optimization pipeline (MPS on Mac, CUDA support)
   - âœ… Benchmarking framework (EVREAL metrics)

2. **Base Algorithms** âœ… COMPLETED
   - âœ… **E2VID**: CNN-based UNet architecture implemented
   - âœ… **FireNet**: Lightweight speed-optimized variant implemented
   - âœ… Model conversion utilities and download scripts

### Phase 2: Enhanced Variants (4-6 weeks) - âœ… COMPLETED

**Priority: MEDIUM**

3. **E2VID+**: Enhanced features and training (2-3 weeks) âœ… COMPLETED

   - âœ… ConvLSTM implementation for temporal processing
   - âœ… E2VID+ architecture with temporal memory
   - âœ… Simplified temporal attention mechanism
   - âœ… Python bindings for E2VID+ (`temporal_reconstruction_demo.py`)
   - âœ… Integration tests with real event data

4. **FireNet+**: Enhanced FireNet with additional features (2-3 weeks) âœ… COMPLETED
   - âœ… FireNet+ lightweight variant with temporal gating
   - âœ… FireModulePlus with temporal processing
   - âœ… Python bindings for FireNet+
   - âœ… Performance benchmarking vs base FireNet

### Phase 3: Advanced Architectures (6-8 weeks) - âœ… COMPLETED

**Priority: MEDIUM**

5. **SPADE-E2VID**: Spatially-adaptive normalization (3-4 weeks) âœ… COMPLETED

   - âœ… SPADE normalization layers (SpadeNorm, SpadeResBlock)
   - âœ… SpadeGenerator for full image synthesis
   - âœ… SpadeE2Vid with full SPADE integration
   - âœ… HybridSpadeE2Vid with learnable path blending
   - âœ… SpadeE2VidLite lightweight variant
   - âœ… Python bindings for SPADE models
   - âœ… Unit tests and integration tests

6. **SSL-E2VID**: Self-supervised approach (3-4 weeks) âœ… COMPLETED
   - âœ… Self-supervised loss functions (ContrastiveLoss, EventReconstructionLoss)
   - âœ… Temporal consistency losses
   - âœ… Contrastive learning framework
   - âœ… SSL trainer with momentum encoder
   - âœ… Event augmentation strategies
   - âœ… Python bindings for SSL models

### Phase 4: Model Infrastructure and Python API (2-4 weeks) - âœ… COMPLETED

**Priority: HIGH - Feature Completion**

7. **Model Loading and Deployment** âœ… COMPLETED

   - âœ… PyTorch weight loading infrastructure (with documented limitations)
     - Implemented placeholder with clear documentation
     - Created PyTorch to ONNX conversion workflow
     - Added pytorch_model_workflow.py guide
   - âœ… Model zoo with automatic downloading infrastructure
   - âœ… Model conversion scripts (pytorch_to_onnx_converter.py)
   - âœ… Model URLs with consistent GitHub releases format
   - âœ… Model metadata with format support (ONNX/PyTorch)
   - ğŸ”² Deployment examples and Docker container (future work)

8. **Comprehensive Python API** âœ… COMPLETED
   - âœ… Unified Python interface for all models
   - âœ… High-level API: `evlib.models.E2VID()`, `evlib.models.SPADE()`, etc.
   - âœ… Model configuration classes with pre-defined configs
   - âœ… Support for all 6 model types (E2VID, FireNet, +variants, SPADE, SSL)
   - âœ… Automatic fallback between ONNX and Candle backends
   - ğŸ”² Batch processing and streaming (future enhancements)

#### Phase 4 Summary (January 2025)

- **Unified Python API**: All 6 models accessible via `evlib.models.*`
- **Model Zoo Infrastructure**: Complete with URL patterns and metadata
- **PyTorch Loading**: Documented limitations and ONNX workaround
- **SPADE/SSL Integration**: Working through unified API
- **Documentation**: Comprehensive examples and workflow guides

**Next Priority**: Upload actual pre-trained models to GitHub releases

### Phase 5: Advanced Research Models (8-12 weeks) - âœ… COMPLETED

**Priority: MEDIUM - Research Focus**

9. **ET-Net**: Transformer-based (4-6 weeks) âœ… COMPLETED

   - âœ… Vision Transformer (ViT) components in Candle
   - âœ… Event-specific positional encoding
   - âœ… Multi-scale temporal attention
   - âœ… Pre-trained model support infrastructure

10. **HyperE2VID**: Dynamic convolutions + hypernetworks (4-6 weeks) âœ… COMPLETED
    - âœ… HyperNetwork implementation
    - âœ… Dynamic kernel generation
    - âœ… Multi-resolution processing
    - âœ… Adaptive computation

### Model Zoo Enhancements (Phase 5 Summary - January 2025)

- âœ… Found and integrated real E2VID model URL with correct checksum
- âœ… ET-Net transformer architecture with patch embedding
- âœ… HyperE2VID with context-aware dynamic convolutions
- âœ… Python wrappers for both new architectures
- âœ… Model info retrieval from Rust via `get_model_info_py`

### Phase 6: PyTorch Weight Loading & Model Conversion Infrastructure (2-4 weeks) - âœ… COMPLETED

**Priority: CRITICAL - Enables use of pre-trained models**

11. **PyTorch Checkpoint Loading** (1-2 weeks) âœ… COMPLETED

    - âœ… PyO3-based bridge to load .pth files using Python's torch
    - âœ… Map PyTorch state_dict keys to Candle variable names
    - âœ… Handle architecture differences between PyTorch and Candle
    - âœ… Support for nested state dicts and module prefixes

12. **Automated ONNX Conversion Pipeline** (1 week) âœ… COMPLETED

    - âœ… Enhanced conversion script with actual E2VID architecture matching
    - âœ… ONNX optimization passes for better inference performance
    - âœ… Model-specific conversion configs for E2VID
    - âœ… Successfully generated 47MB E2VID ONNX model

13. **Model Verification Framework** (1 week) âœ… COMPLETED
    - âœ… Compare outputs between PyTorch and Candle versions
    - âœ… Visual quality metrics for reconstruction (PSNR, SSIM, RMSE)
    - âœ… Performance benchmarks for inference speed
    - âœ… Automated testing for model compatibility

14. **Unified Model Loading System** (remaining) ğŸ”§ IN PROGRESS
    - ğŸ”² Seamless support for .pth, .onnx, and .safetensors formats
    - ğŸ”² Automatic format detection and appropriate loader selection
    - ğŸ”² Unified API for all model formats

### Phase 7: Advanced Model Architectures (6-8 weeks) ğŸš€ FUTURE

**Priority: MEDIUM - Next-generation models**

14. **E2VIDiff - Diffusion Models** (3-4 weeks)

    - ğŸ”² Denoising diffusion models for event reconstruction
    - ğŸ”² Temporal consistency constraints
    - ğŸ”² High-resolution output support
    - ğŸ”² Conditional generation with event guidance

15. **Recurrent Vision Transformer (RViT)** (2-3 weeks)

    - ğŸ”² Combine transformer with recurrent memory
    - ğŸ”² Better handling of long event sequences
    - ğŸ”² Adaptive temporal resolution
    - ğŸ”² Memory-efficient attention mechanisms

16. **Neural Radiance Fields (NeRF) for Events** (2-3 weeks)
    - ğŸ”² 3D scene reconstruction from events
    - ğŸ”² Novel view synthesis
    - ğŸ”² Integration with SLAM systems
    - ğŸ”² Real-time rendering pipeline

### Phase 8: Real-time Processing & Optimization (4-6 weeks) âš¡ PERFORMANCE

**Priority: HIGH - Production deployment**

17. **Streaming Processing Pipeline** (2-3 weeks)

    - ğŸ”² Process events in real-time as they arrive
    - ğŸ”² Sliding window reconstruction
    - ğŸ”² Adaptive quality based on computational budget
    - ğŸ”² Buffer management and frame dropping

18. **Hardware Acceleration** (2-3 weeks)

    - ğŸ”² CUDA kernel optimizations for voxel grid generation
    - ğŸ”² Metal Performance Shaders for macOS
    - ğŸ”² WebGPU support for browser deployment
    - ğŸ”² SIMD optimizations for CPU processing

19. **Model Quantization & Pruning** (1-2 weeks)
    - ğŸ”² INT8 quantization for faster inference
    - ğŸ”² Structured pruning for mobile deployment
    - ğŸ”² Knowledge distillation for smaller models
    - ğŸ”² Dynamic quantization based on content

### Phase 9: Application Frameworks (6-8 weeks) ğŸ“± APPLICATIONS

**Priority: MEDIUM - End-user features**

20. **Event-based Video Processing** (2-3 weeks)

    - ğŸ”² Video stabilization using events
    - ğŸ”² HDR video reconstruction
    - ğŸ”² Motion deblurring
    - ğŸ”² Frame interpolation

21. **Robotics Integration** (2-3 weeks)

    - ğŸ”² ROS2 nodes for event processing
    - ğŸ”² Visual odometry and SLAM
    - ğŸ”² Object tracking and detection
    - ğŸ”² Obstacle avoidance

22. **Scientific Applications** (2-3 weeks)
    - ğŸ”² Astronomy (fast-moving objects)
    - ğŸ”² Microscopy (high-speed phenomena)
    - ğŸ”² Particle physics visualization
    - ğŸ”² Biomedical imaging

### Phase 10: Ecosystem & Tools (4-6 weeks) ğŸ› ï¸ DEVELOPER EXPERIENCE

**Priority: HIGH - Community adoption**

23. **GUI Application** (2-3 weeks)

    - ğŸ”² Real-time visualization of reconstructions
    - ğŸ”² Model comparison tools
    - ğŸ”² Dataset annotation interface
    - ğŸ”² Performance profiling

24. **Cloud Deployment** (2-3 weeks)

    - ğŸ”² REST API for model inference
    - ğŸ”² Batch processing on cloud GPUs
    - ğŸ”² Model serving with auto-scaling
    - ğŸ”² Docker and Kubernetes configs

25. **Educational Resources** (1-2 weeks)
    - ğŸ”² Interactive Jupyter notebooks
    - ğŸ”² Video tutorials
    - ğŸ”² Benchmark datasets with ground truth
    - ğŸ”² Course materials

### Phase 11: Ecosystem Integration (4-8 weeks) ğŸŒ ECOSYSTEM

**Priority: MEDIUM - External compatibility**

26. **Video-to-Events (V2E) Simulation** (3-4 weeks)

    - ğŸ”² ESIM (Event Simulator) implementation
    - ğŸ”² V2E conversion algorithms
    - ğŸ”² Noise models and camera parameters
    - ğŸ”² Integration with existing datasets

27. **External Tool Integration** (4-6 weeks)
    - ğŸ”² DV Processing compatibility layer
    - ğŸ”² OpenEB format support and HAL integration
    - ğŸ”² Prophesee Metavision SDK compatibility
    - ğŸ”² ROS/ROS2 nodes for real-time processing

## Immediate Next Steps (1-2 weeks)

1. **Model Zoo Infrastructure**

   ```rust
   // models/model_zoo.rs
   pub struct ModelZoo {
       models: HashMap<String, ModelInfo>,
       cache_dir: PathBuf,
   }
   ```

2. **Unified Python API**

   ```python
   import evlib.models as models

   # Simple API
   model = models.E2VID(variant="unet", pretrained=True)
   frames = model.reconstruct(events)

   # Advanced API
   model = models.SPADE(
       config=models.SpadeConfig(
           num_layers=4,
           base_channels=64,
           spade_layers=[2, 3]
       )
   )
   ```

3. **Pre-trained Model Support**

   - Download scripts for all implemented models
   - Automatic weight conversion from PyTorch to Candle
   - Model validation and testing

4. **Documentation and Examples**
   - Jupyter notebook for each model architecture
   - Performance comparison notebook
   - Real-time demo applications

## Technical Debt and Maintenance

1. **Code Quality**

   - ğŸ”² Increase test coverage to >90%
   - ğŸ”² Add property-based testing
   - ğŸ”² Performance regression tests

2. **Documentation**

   - ğŸ”² API reference documentation
   - ğŸ”² Architecture diagrams
   - ğŸ”² Contributing guidelines

3. **CI/CD Improvements**
   - ğŸ”² GPU testing in CI
   - ğŸ”² Automated benchmarking
   - ğŸ”² Model validation tests

## Success Metrics

- **Performance**: All models achieve real-time performance (>30 FPS) on modern GPUs
- **Accuracy**: Match or exceed original paper results on standard benchmarks
- **Usability**: <5 lines of code to load and use any model
- **Coverage**: Support all major event-to-video reconstruction algorithms
- **Community**: Active contributors and users, integrated into research workflows

## Notes

- All implementations prioritize performance through Rust while maintaining Python ease-of-use
- Candle framework provides the foundation for all neural network implementations
- Focus on practical deployment and real-world usage scenarios
- Maintain compatibility with existing event camera ecosystems

## Current Development Focus (Option A: Research-Ready - 8-10 weeks)

**ğŸ¯ PRIORITY PATH FOR CORE FUNCTIONALITY**

### âœ… Phase 6 - NEARLY COMPLETED (95% done)
- PyTorch weight loading infrastructure âœ…
- ONNX conversion pipeline âœ…
- Model verification framework âœ…
- **Remaining**: Unified model loading system (1 week)

### ğŸ”§ Phase 8 - NEXT PRIORITY (4-6 weeks)
**Real-time Processing & Optimization** - Critical for production use
- Streaming processing pipeline
- Hardware acceleration (CUDA/Metal/WebGPU)
- Model quantization & pruning

### ğŸ“š Documentation & Testing (2-3 weeks)
- Comprehensive test coverage >90%
- API documentation
- Performance benchmarks

**Total Timeline: 8-10 weeks for research-ready library**

## Future Work (Option B: Production-Ready)
- Phase 9: Application Frameworks (6-8 weeks)
- Phase 10: Developer Tools & Ecosystem (4-6 weeks)
- Phase 11: External Integration (4-8 weeks)

### Technical Status

âœ… **SOLVED**: PyTorch Weight Loading
- Implemented PyO3-based bridge for loading .pth files
- Complete tensor conversion PyTorch â†’ Candle
- Model weight mapping for all architectures
- Enhanced ONNX conversion with proper architecture matching
- Comprehensive verification framework

ğŸ”§ **IN PROGRESS**: Unified Model Loading
- Seamless .pth/.onnx/.safetensors support
- Automatic format detection

ğŸš€ **NEXT**: Real-time Processing Pipeline
- Critical for production deployment and 30+ FPS performance goals
