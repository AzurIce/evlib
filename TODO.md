I ultimately want evlib to be the one stop shop for event based vision utilities
as well as a place where many of the state of the art algorithms have been
implemented and reside.

Events to Video algorithms (vid2e) I would like to implement are:

- [E2VID](https://github.com/uzh-rpg/rpg_e2vid)
- [FireNet](https://github.com/cedric-scheerlinck/rpg_e2vid/tree/cedric/firenet)
- [E2VID+](https://github.com/TimoStoff/event_cnn_minimal)
- [FireNet+](https://github.com/TimoStoff/event_cnn_minimal)
- [SPADE-E2VID](https://github.com/RodrigoGantier/SPADE_E2VID)
- [SSL-E2VID](https://github.com/tudelft/ssl_e2vid)
- [ET-Net](https://github.com/WarranWeng/ET-Net)
- [HyperE2VID](https://github.com/ercanburak/HyperE2VID)

They all should use Candle framework to re-implement the architecture and use
existing pytorch model files where possible. We can leverage either the existing
PyTorch loader (pytorch_loader.rs) or ONNX Runtime (ort) for model inference.

https://github.com/ercanburak/EVREAL does a good job of comparing the different
algorithms and points to many good resources. I would like all of these
algorithms to have their implementation at: `evlib/src/ev_processing/reconstruction/`

After that my plan is to implement the functionality found here that does the
reverse, i.e Video to Events and works as a simulator. The main repo for this
can be found here: https://github.com/uzh-rpg/rpg_e2vid

A useful set of open-source tooling can be found here:
https://gitlab.com/inivation/dv/dv-processing. I would like to eventually
incorporate the functionality there inside evlib too.

A project that is aiming to do similar things is
https://github.com/ucsd-hdsi-dvs/V2CE-Toolbox, maybe you can draw inspiration
from how they do things.

Of course there is also https://github.com/prophesee-ai/openeb too and
https://github.com/shiba24/event-vision-library and actually there is a good
amount of tooling found here too: https://github.com/tub-rip/ETAP

## Implementation Status

### Recent Progress (January 2025)
- âœ… Implemented E2VID UNet and FireNet architectures in Candle
- âœ… Integrated ONNX Runtime (ort v2.0.0-rc.9) for model inference
- âœ… Added Python API with model selection (unet/firenet/onnx/simple)
- âœ… Comprehensive test suite for both Rust and Python APIs
- âœ… Successfully tested with slider_depth dataset
- âœ… PyTorch to ONNX model converter with validation
- âœ… GPU optimization utilities (CUDA/Metal support)
- âœ… EVREAL benchmarking metrics (MSE, PSNR, SSIM, MS-SSIM)
- âœ… Temporal consistency metrics for video sequences

## Implementation Plan

### Phase 1: Foundation (4-6 weeks) - âœ… COMPLETED

**Priority: HIGH**

1. **Core Infrastructure** âœ… COMPLETED
   - âœ… ONNX Runtime (ort) integration
   - âœ… Enhanced voxel grid representations
   - âœ… Candle-based E2VID UNet and FireNet architectures
   - âœ… PyTorch to ONNX model converter
   - âœ… GPU optimization pipeline (MPS on Mac, CUDA support)
   - âœ… Benchmarking framework (EVREAL metrics)

2. **Base Algorithms** âœ… COMPLETED
   - âœ… **E2VID**: CNN-based UNet architecture implemented
   - âœ… **FireNet**: Lightweight speed-optimized variant implemented
   - âœ… Model conversion utilities and download scripts

### Phase 2: Enhanced Variants (4-6 weeks) ğŸš§ IN PROGRESS

**Priority: MEDIUM**

3. **E2VID+**: Enhanced features and training (2-3 weeks) âœ… COMPLETED
   - âœ… ConvLSTM implementation for temporal processing
   - âœ… E2VID+ architecture with temporal memory
   - âœ… Simplified temporal attention mechanism
   - ğŸ”² Python bindings for E2VID+
   - ğŸ”² Integration tests with real event data

4. **FireNet+**: Enhanced FireNet with additional features (2-3 weeks) âœ… COMPLETED
   - âœ… FireNet+ lightweight variant with temporal gating
   - âœ… FireModulePlus with temporal processing
   - ğŸ”² Python bindings for FireNet+
   - ğŸ”² Performance benchmarking vs base FireNet

### Phase 3: Advanced Architectures (6-8 weeks) ğŸš§ IN PROGRESS

**Priority: MEDIUM**

5. **SPADE-E2VID**: Spatially-adaptive normalization (3-4 weeks) âœ… COMPLETED
   - âœ… SPADE normalization layers (SpadeNorm, SpadeResBlock)
   - âœ… SpadeGenerator for full image synthesis
   - âœ… SpadeE2Vid with full SPADE integration
   - âœ… HybridSpadeE2Vid with learnable path blending
   - âœ… SpadeE2VidLite lightweight variant
   - ğŸ”² Python bindings for SPADE models
   - ğŸ”² Pre-trained model support

6. **SSL-E2VID**: Self-supervised approach (3-4 weeks)
   - ğŸ”² Self-supervised loss functions
   - ğŸ”² Temporal consistency losses
   - ğŸ”² Contrastive learning framework

### Phase 4: Cutting-Edge Research (8-12 weeks)

**Priority: LOW - Long-term**

7. **ET-Net**: Transformer-based (4-6 weeks)
8. **HyperE2VID**: Dynamic convolutions + hypernetworks (4-6 weeks)

### Algorithm Complexity Analysis

**Low Effort (1-2 weeks):** âœ… COMPLETED

- âœ… E2VID basic implementation
- âœ… FireNet implementation
- â³ Model loading from PyTorch checkpoints

**Medium Effort (3-4 weeks):**

- E2VID+ with enhanced features
- SPADE-E2VID (requires custom SPADE layers)
- ConvLSTM implementation for temporal processing

**High Effort (1-2 months):**

- ET-Net (transformer architecture)
- SSL-E2VID (self-supervised training)
- HyperE2VID (dynamic convolutions + hypernetworks)

### Implementation Notes

**Candle Framework Capabilities:**

- CNN layers available (conv2d, batch norm, layer norm)
- Limited transformer and ConvLSTM support (will need custom implementation)

**Completed Components:**

- âœ… E2VID UNet architecture in Candle (e2vid_arch.rs)
- âœ… FireNet architecture in Candle (e2vid_arch.rs)
- âœ… ONNX Runtime integration (onnx_loader_simple.rs)
- âœ… Python API with model selection (events_to_video_advanced)
- âœ… Basic PyTorch loader infrastructure (pytorch_loader.rs)

**Remaining Components:**

- â³ Loading actual pre-trained weights (.pth files)
- â³ GPU optimization (CUDA/Metal providers)
- âœ… ConvLSTM layers for temporal processing
- âœ… SPADE normalization layers
- â³ Benchmark metrics implementation

**Benchmark Framework (EVREAL):**

- MSE, SSIM, LPIPS (full-reference metrics)
- BRISQUE, NIQE, MANIQA (no-reference metrics)

**Dataset Support:**

- ğŸ”² ECD (Event Camera Dataset)
- ğŸ”² MVSEC (Multi Vehicle Stereo Event Camera)
- ğŸ”² HQF (High Quality Frames)
- ğŸ”² BS-ERGB (Beam Splitter Event-RGB)
- ğŸ”² HDR (High Dynamic Range)
